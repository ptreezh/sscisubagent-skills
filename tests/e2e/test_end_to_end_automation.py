#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶
å®Œæ•´æµ‹è¯•æ•´ä¸ªæ™ºèƒ½åŒ–éƒ¨ç½²å’Œä½¿ç”¨æµç¨‹
"""

import unittest
import sys
import os
import json
import subprocess
import tempfile
import shutil
import time
import threading
import requests
from pathlib import Path
from unittest.mock import patch, MagicMock
import signal

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

class TestEndToEndAutomation(unittest.TestCase):
    """ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = Path(tempfile.mkdtemp())
        self.original_dir = Path.cwd()
        self.results = {}

        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        self.setup_test_environment()

        # åˆ‡æ¢åˆ°æµ‹è¯•ç›®å½•
        os.chdir(self.test_dir)

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        os.chdir(self.original_dir)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for file_path in self.results.get("temp_files", []):
            try:
                if file_path.is_file():
                    file_path.unlink()
                elif file_path.is_dir():
                    shutil.rmtree(file_path, ignore_errors=True)
            except Exception:
                pass

        shutil.rmtree(self.test_dir, ignore_errors=True)

    def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # åˆ›å»ºå®Œæ•´çš„SSCIé¡¹ç›®ç»“æ„
        self.create_ssci_project_structure()

        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºæ¸…ç†
        self.results["temp_files"] = []

    def create_ssci_project_structure(self):
        """åˆ›å»ºSSCIé¡¹ç›®ç»“æ„"""
        # åˆ›å»ºä¸»è¦ç›®å½•
        (self.test_dir / "skills").mkdir()
        (self.test_dir / "tests").mkdir()
        (self.test_dir / "uploads").mkdir()
        (self.test_dir / "results").mkdir()

        # åˆ›å»ºä¾èµ–é…ç½®æ–‡ä»¶
        (self.test_dir / "pyproject.toml").write_text("""
[project]
name = "ssci-subagent-skills"
version = "1.0.0"
dependencies = [
    "jieba>=0.42.0",
    "networkx>=3.0.0",
    "pandas>=1.5.0",
    "numpy>=1.20.0"
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "flask>=2.0.0",
    "flask-cors>=3.0.0"
]
        """, encoding='utf-8')

        (self.test_dir / "requirements.txt").write_text("""
jieba>=0.42.0
networkx>=3.0.0
pandas>=1.5.0
numpy>=1.20.0
        """, encoding='utf-8')

        # åˆ›å»ºçœŸå®æŠ€èƒ½
        self.create_realistic_skills()

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self.create_test_data()

    def create_realistic_skills(self):
        """åˆ›å»ºçœŸå®çš„æŠ€èƒ½ç»“æ„"""
        skills_config = [
            {
                "path": "skills/coding/open-coding",
                "description": "ä¸­æ–‡å¼€æ”¾ç¼–ç åˆ†æå·¥å…·",
                "scripts": [
                    {"name": "preprocess.py", "args": ["--input", "input.txt", "--output", "output.json"]},
                    {"name": "extract_concepts.py", "args": ["--input", "input.json", "--output", "concepts.json"]},
                    {"name": "compare_codes.py", "args": ["--input", "codes.json", "--output", "optimized.json"]}
                ]
            },
            {
                "path": "skills/analysis/centrality-analysis",
                "description": "ç½‘ç»œä¸­å¿ƒæ€§åˆ†æå·¥å…·",
                "scripts": [
                    {"name": "centrality.py", "args": ["--input", "network.json", "--output", "report.json"]}
                ]
            },
            {
                "path": "skills/coding/theory-saturation",
                "description": "ç†è®ºé¥±å’Œåº¦æ£€éªŒå·¥å…·",
                "scripts": [
                    {"name": "assess_saturation.py", "args": ["--data-dir", "data/", "--output", "saturation.json"]}
                ]
            }
        ]

        for skill_config in skills_config:
            skill_path = Path(skill_config["path"])
            skill_path.mkdir(parents=True)

            # åˆ›å»ºSKILL.md
            (skill_path / "SKILL.md").write_text(
                f"""---
name: {skill_config["path"].replace("/", "-")}
description: {skill_config["description"]}
---

# {skill_config["description"]}

## ä½¿ç”¨æ–¹æ³•

é€šè¿‡Webç•Œé¢ä¸Šä¼ æ•°æ®æ–‡ä»¶æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·è¿›è¡Œæ•°æ®åˆ†æã€‚
                """,
                encoding='utf-8'
            )

            # åˆ›å»ºscriptsç›®å½•
            scripts_dir = skill_path / "scripts"
            scripts_dir.mkdir()

            # åˆ›å»ºçœŸå®è„šæœ¬
            for script in skill_config["scripts"]:
                script_path = scripts_dir / script["name"]
                self.create_real_script(script_path, skill_config["description"])

    def create_real_script(self, script_path: Path, description: str):
        """åˆ›å»ºçœŸå®çš„å¯æ‰§è¡Œè„šæœ¬"""
        script_name = script_path.stem
        script_content = f"""#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
{description} çœŸå®å®ç°è„šæœ¬

import json
import sys
import argparse
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description='{description}è„šæœ¬')
    for arg in {script_args}:
        parser.add_argument(arg)

    args = parser.parse_args()

    # æ¨¡æ‹ŸçœŸå®çš„å¤„ç†é€»è¾‘
    result = {{
        "status": "success",
        "script": "{script_name}",
        "timestamp": "2023-12-16T10:00:00Z",
        "processed": True,
        "details": "E2Eæµ‹è¯•å¤„ç†å®Œæˆ",
        "args": vars(args)
    }}

    # å¤„ç†ç‰¹å®šè„šæœ¬é€»è¾‘
    if hasattr(args, 'output') and args.output:
        result["output_file"] = str(args.output)
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

    if hasattr(args, 'input') and args.input:
        input_path = Path(args.input)
        if input_path.exists():
            result["input_file"] = str(input_path)
            result["input_size"] = input_path.stat().st_size
            result["input_content"] = input_path.read_text(encoding='utf-8')[:100]  # å‰100ä¸ªå­—ç¬¦

    if hasattr(args, 'data_dir') and args.data_dir:
        data_dir = Path(args.data_dir)
        if data_dir.exists():
            result["data_dir"] = str(data_dir)
            result["data_files"] = [f.name for f in data_dir.iterdir() if f.is_file()]

    # è¾“å‡ºç»“æœ
    print(f"âœ… {{script_name}} æ‰§è¡ŒæˆåŠŸ!")
    if result.get("output_file"):
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜: {{result['output_file']}}")

    return result

if __name__ == '__main__':
    try:
        result = main()
        if result["status"] == "error":
            print(f"âŒ æ‰§è¡Œå¤±è´¥: {{result.get('error', 'æœªçŸ¥é”™è¯¯')}}")
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¼‚å¸¸: {{e}}")
        sys.exit(1)
        """
                .format(
                    script_name=script_name,
                    script_args=", ".join([
                        f'"{arg}"'
                        for arg in script_config["args"]
                    ])
                )
            )
        )

        # ç¡®ä¿è„šæœ¬å¯æ‰§è¡Œ
        script_path.chmod(0o755)

        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
        self.results["temp_files"].append(script_path)

    def create_test_data(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        # åˆ›å»ºè®¿è°ˆæ–‡æœ¬æ•°æ®
        interview_dir = self.test_dir / "test_data" / "interviews"
        interview_dir.mkdir(parents=True)

        interview_texts = [
            "æˆ‘åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°äº†å¾ˆå¤šå›°éš¾ã€‚æœ‰æ—¶å€™ä½œä¸šå¾ˆéš¾å®Œæˆï¼Œæˆ‘ä¸çŸ¥é“è¯¥å‘è°æ±‚åŠ©ã€‚",
            "æœ‰ä¸€æ¬¡æˆ‘é¼“èµ·å‹‡æ°”å‘è€å¸ˆè¯·æ•™ï¼Œè€å¸ˆå¾ˆè€å¿ƒåœ°ç»™æˆ‘è®²è§£äº†é—®é¢˜ã€‚",
            "ä»é‚£ä»¥åï¼Œæˆ‘å¼€å§‹ä¸»åŠ¨å¯»æ±‚å¸®åŠ©ã€‚åŒå­¦ä»¬ä¹‹é—´ä¹Ÿå¾ˆé‡è¦ï¼Œæˆ‘ä»¬ç»å¸¸ä¸€èµ·è®¨è®ºé—®é¢˜ã€‚",
            "æˆ‘è§‰å¾—æ•™å­¦è´¨é‡å¯¹å­¦ä¹ æ•ˆæœå½±å“å¾ˆå¤§ã€‚å¥½çš„è€å¸ˆè®²è¯¾å¾ˆç”ŸåŠ¨ï¼Œèƒ½è®©å­¦ç”Ÿå¾ˆå®¹æ˜“ç†è§£ã€‚"
        ]

        for i, text in enumerate(interview_texts, 1):
            (interview_dir / f"interview_{i:02d}.txt").write_text(text, encoding='utf-8')

        # åˆ›å»ºç½‘ç»œæ•°æ®
        network_dir = self.test_dir / "test_data" / "networks"
        network_dir.mkdir(parents=True)

        network_data = {
            "nodes": ["å­¦ç”ŸA", "å­¦ç”ŸB", "å­¦ç”ŸC", "è€å¸ˆ", "å®¶é•¿A", "å®¶é•¿B"],
            "edges": [
                ["å­¦ç”ŸA", "å­¦ç”ŸB", 3],
                ["å­¦ç”ŸA", "å­¦ç”ŸC", 2],
                ["å­¦ç”ŸB", "è€å¸ˆ", 4],
                ["å­¦ç”ŸC", "è€å¸ˆ", 3],
                ["å­¦ç”ŸA", "å®¶é•¿A", 2],
                ["å­¦ç”ŸB", "å®¶é•¿B", 1]
            ]
        }

        (network_dir / "classroom_network.json").write_text(
            json.dumps(network_data, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )

        # åˆ›å»ºç¼–ç æ•°æ®
        coding_dir = self.test_dir / "test_data" / "coding"
        coding_dir.mkdir(parents=True)

        codes_data = {
            "codes": [
                {"code": "å¯»æ±‚å¸®åŠ©", "frequency": 15, "type": "è¡ŒåŠ¨æ¦‚å¿µ"},
                {"code": "è·å¾—æ”¯æŒ", "frequency": 12, "type": "è¡ŒåŠ¨æ¦‚å¿µ"},
                {"code": "å¸ˆç”Ÿå…³ç³»", "frequency": 10, "type": "å…³ç³»æ¦‚å¿µ"},
                {"code": "å­¦ä¹ å›°éš¾", "frequency": 8, "type": "é—®é¢˜æ¦‚å¿µ"}
            ]
        }

        (coding_dir / "existing_codes.json").write_text(
            json.dumps(codes_data, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )

    def test_01_complete_deployment_workflow(self):
        """æµ‹è¯•1: å®Œæ•´éƒ¨ç½²å·¥ä½œæµ"""
        # Given
        from smart_deploy import SmartDeployer
        deployer = SmartDeployer()

        # When - æ‰§è¡Œå®Œæ•´éƒ¨ç½²æµç¨‹
        # 1. ç³»ç»Ÿè¯Šæ–­
        diagnostics = deployer.run_diagnostics()
        self.assertIsInstance(diagnostics, dict)
        self.assertIn("system", diagnostics)
        self.assertIn("skills", diagnostics)

        # 2. éªŒè¯æŠ€èƒ½å‘ç°
        skills = deployer.available_skills
        self.assertGreater(len(skills), 0)

        # 3. ç”Ÿæˆä½¿ç”¨æŒ‡å—
        guide = deployer.generate_usage_guide()
        self.assertIsInstance(guide, str)
        self.assertIn("ä½¿ç”¨æŒ‡å—", guide)

        # 4. åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬
        script = deployer.create_quick_start_script()
        self.assertIsInstance(script, str)
        self.assertIn("python", script)

        # Then - æ‰€æœ‰æ­¥éª¤éƒ½åº”è¯¥æˆåŠŸ
        self.assertTrue(True)

    def test_02_skills_launcher_interactive_workflow(self):
        """æµ‹è¯•2: æŠ€èƒ½å¯åŠ¨å™¨äº¤äº’å·¥ä½œæµ"""
        # Given
        from skills_launcher import SkillsLauncher

        # When - æ¨¡æ‹Ÿäº¤äº’æµç¨‹
        launcher = SkillsLauncher()

        # 1. éªŒè¯æŠ€èƒ½åŠ è½½
        skills = launcher.skills
        self.assertIsInstance(skills, dict)

        # 2. éªŒè¯æ¬¢è¿ç•Œé¢
        with patch('builtins.input') as mock_input:
            mock_input.return_value = "0"  # é€€å‡º

            with patch('sys.stdout', new_callable=lambda x: io.StringIO()) as mock_stdout:
                launcher.interactive_menu()

            # éªŒè¯è¾“å‡ºåŒ…å«é¢„æœŸå†…å®¹
                output = mock_stdout.getvalue()
                self.assertIn("SSCIæŠ€èƒ½åŒ…", output)

        # 3. éªŒè¯å†å²ç®¡ç†
        initial_history_len = len(launcher.history)
        launcher._save_history({
            "skill": "test-skill",
            "script": "test.py",
            "timestamp": "2023-01-01",
            "success": True
        })
        self.assertEqual(len(launcher.history), initial_history_len + 1)

        # Then - äº¤äº’åŠŸèƒ½æ­£å¸¸
        self.assertTrue(True)

    def test_03_web_interface_complete_workflow(self):
        """æµ‹è¯•3: Webç•Œé¢å®Œæ•´å·¥ä½œæµ"""
        # Given
        from web_interface import WEB_AVAILABLE

        if not WEB_AVAILABLE:
            self.skipTest("Web dependencies not available")

        from web_interface import WebInterface

        # When - æµ‹è¯•Webç•Œé¢å·¥ä½œæµ
        # 1. åˆ›å»ºWebåº”ç”¨
        web_interface = WebInterface(port=5003)
        app = web_interface.create_app()
        self.assertIsNotNone(app)

        # 2. åˆ›å»ºæ¨¡æ¿
        web_interface.create_templates()
        templates_dir = web_interface.root_dir / "templates"
        self.assertTrue(templates_dir.exists())

        # 3. æµ‹è¯•APIç«¯ç‚¹
        with app.test_client() as client:
            # æµ‹è¯•æŠ€èƒ½API
            response = client.get('/api/skills')
            self.assertEqual(response.status_code, 200)

            # æµ‹è¯•ç»“æœAPI
            response = client.get('/api/results')
            self.assertEqual(response.status_code, 200)

        # 4. æµ‹è¯•é™æ€èµ„æº
        base_template = templates_dir / "base.html"
        self.assertTrue(base_template.exists())

        # Then - Webç•Œé¢åŠŸèƒ½æ­£å¸¸
        self.assertTrue(True)

    def test_04_script_execution_through_web_interface(self):
        """æµ‹è¯•4: é€šè¿‡Webç•Œé¢æ‰§è¡Œè„šæœ¬"""
        # Given
        from web_interface import WEB_AVAILABLE

        if not WEB_AVAILABLE:
            self.skipTest("Web dependencies not available")

        from web_interface import WebInterface
        web_interface = WebInterface(port=5004)
        app = web_interface.create_app()

        # åˆ›å»ºæµ‹è¯•è¾“å…¥æ–‡ä»¶
        test_file = self.test_dir / "test_input.txt"
        test_file.write_text("æµ‹è¯•å†…å®¹", encoding='utf-8')
        self.results["temp_files"].append(test_file)

        # When - é€šè¿‡Webç•Œé¢æ‰§è¡Œè„šæœ¬
        with app.test_client() as client:
            # ä½¿ç”¨multipart/form-dataæ ¼å¼ä¸Šä¼ æ–‡ä»¶
            with open(test_file, 'rb') as f:
                response = client.post(
                    '/run/open-coding/preprocess.py',
                    data={'file': (f, 'test_input.txt')},
                    content_type='multipart/form-data'
                )

        # Then - åº”è¯¥èƒ½å¤„ç†ä¸Šä¼ è¯·æ±‚
        self.assertIn(response.status_code, [200, 400, 500])

    def test_05_data_processing_pipeline(self):
        """æµ‹è¯•5: æ•°æ®å¤„ç†ç®¡é“"""
        # Given
        interview_file = self.test_dir / "test_data" / "interviews" / "interview_01.txt"
        network_file = self.test_dir / "test_data" / "networks" / "classroom_network.json"
        codes_file = self.test_dir / "test_data" / "coding" / "existing_codes.json"

        # When - æ‰§è¡Œæ•°æ®å¤„ç†ç®¡é“

        # 1. æ–‡æœ¬é¢„å¤„ç†
        preprocess_script = self.test_dir / "skills" / "coding" / "open-coding" / "scripts" / "preprocess.py"
        if preprocess_script.exists():
            result = subprocess.run([
                sys.executable, str(preprocess_script),
                "--input", str(interview_file),
                "--output", str(interview_file.parent / f"{interview_file.stem}_preprocessed.json")
            ], capture_output=True, text=True)

            self.assertEqual(result.returncode, 0)
            self.assertIn("æ‰§è¡ŒæˆåŠŸ", result.stdout)

        # 2. ç½‘ç»œåˆ†æ
        centrality_script = self.test_dir / "skills" / "analysis" / "centrality-analysis" / "scripts" / "centrality.py"
        if centrality_script.exists():
            result = subprocess.run([
                sys.executable, str(centrality_script),
                "--input", str(network_file),
                "--output", str(network_file.parent / f"{network_file.stem}_report.json")
            ], capture_output=True, text=True)

            self.assertEqual(result.returncode, 0)
            self.assertIn("æ‰§è¡ŒæˆåŠŸ", result.stdout)

        # 3. é¥±å’Œåº¦æ£€éªŒ
        saturation_script = self.test_dir / "skills" / "coding" / "theory-saturation" / "scripts" / "assess_saturation.py"
        if saturation_script.exists():
            result = subprocess.run([
                sys.executable, str(saturation_script),
                "--data-dir", str(codes_file.parent),
                "--output", str(codes_file.parent / f"{codes_file.stem}_report.json")
            ], capture_output=True, text=True)

            self.assertEqual(result.returncode, 0)
            self.assertIn("æ‰§è¡ŒæˆåŠŸ", result.stdout)

        # Then - æ•°æ®å¤„ç†ç®¡é“åº”è¯¥æˆåŠŸ
        self.assertTrue(True)

    def test_06_output_format_validation(self):
        """æµ‹è¯•6: è¾“å‡ºæ ¼å¼éªŒè¯"""
        # Given
        # é¢„æœŸçš„è¾“å‡ºæ–‡ä»¶
        expected_outputs = [
            self.test_dir / "test_data" / "interviews" / "interview_01_preprocessed.json",
            self.test_dir / "test_data" / "networks" / "classroom_network_report.json",
            self.test_dir / "test_data" / "coding" / "existing_codes_report.json"
        ]

        # When - æ‰§è¡Œè„šæœ¬ç”Ÿæˆè¾“å‡º
        self.test_05_data_processing_pipeline()

        # Then - éªŒè¯è¾“å‡ºæ–‡ä»¶æ ¼å¼
        for output_file in expected_outputs:
            if output_file.exists():
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.assertIsInstance(data, dict)
                    self.assertIn("status", data)

    def test_07_error_handling_and_recovery(self):
        """æµ‹è¯•7: é”™è¯¯å¤„ç†å’Œæ¢å¤"""
        # Given
        # åˆ›å»ºæ— æ•ˆè¾“å…¥æ–‡ä»¶
        invalid_file = self.test_dir / "invalid_input.txt"
        invalid_file.write_text("æ— æ•ˆå†…å®¹", encoding='utf-8')
        self.results["temp_files"].append(invalid_file)

        # åˆ›å»ºæœ‰é—®é¢˜çš„è„šæœ¬
        problematic_script = self.test_dir / "problematic.py"
        problematic_script.write_text("""
import sys
sys.exit(1)  # æ¨¡æ‹Ÿé”™è¯¯
        """, encoding='utf-8')

        # When
        result = subprocess.run(
            [sys.executable, str(problematic_script)],
            capture_output=True,
            text=True
        )

        # Then
        self.assertEqual(result.returncode, 1)

    def test_08_performance_benchmarking(self):
        """æµ‹è¯•8: æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        # Given
        import time

        # åˆ›å»ºæ€§èƒ½æµ‹è¯•æ•°æ®
        large_text_file = self.test_dir / "large_text.txt"
        large_text_file.write_text("æµ‹è¯•å†…å®¹\n" * 1000, encoding='utf-8')
        self.results["temp_files"].append(large_text_file)

        preprocess_script = self.test_dir / "skills" / "coding" / "open-coding" / "scripts" / "preprocess.py"

        if preprocess_script.exists():
            # When - æµ‹è¯•æ‰§è¡Œæ—¶é—´
            start_time = time.time()
            result = subprocess.run([
                sys.executable, str(preprocess_script),
                "--input", str(large_text_file),
                "--output", str(large_text_file.parent / "benchmark_output.json")
            ], capture_output=True, text=True)
            end_time = time.time()

            execution_time = end_time - start_time

            # Then
            self.assertEqual(result.returncode, 0)
            self.assertLess(execution_time, 30.0)  # åº”è¯¥åœ¨30ç§’å†…å®Œæˆ

    def test_09_concurrent_usage_simulation(self):
        """æµ‹è¯•9: å¹¶å‘ä½¿ç”¨æ¨¡æ‹Ÿ"""
        # Given
        import threading

        # åˆ›å»ºå¤šä¸ªè¾“å…¥æ–‡ä»¶
        input_files = []
        for i in range(3):
            input_file = self.test_dir / f"concurrent_input_{i}.txt"
            input_file.write_text(f"å¹¶å‘æµ‹è¯•å†…å®¹ {i}", encoding='utf-8')
            input_files.append(input_file)
            self.results["temp_files"].extend(input_files)

        preprocess_script = self.test_dir / "skills" / "coding" / "open-coding" / "scripts" / "preprocess.py"

        if preprocess_script.exists():
            # When - å¹¶å‘æ‰§è¡Œè„šæœ¬
            threads = []
            results = []

            def execute_script(input_file):
                result = subprocess.run([
                    sys.executable, str(preprocess_script),
                    "--input", str(input_file),
                    "--output", str(input_file.parent / f"{input_file.stem}_concurrent.json")
                ], capture_output=True, text=True)
                results.append(result)

            for input_file in input_files:
                thread = threading.Thread(target=execute_script, args=(input_file,))
                threads.append(thread)
                thread.start()

            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in threads:
                thread.join()

            # Then
            success_count = sum(1 for r in results if r.returncode == 0)
            self.assertEqual(success_count, len(input_files))

    def test_10_file_cleanup_mechanism(self):
        """æµ‹è¯•10: æ–‡ä»¶æ¸…ç†æœºåˆ¶"""
        # Given
        # å·²åˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶
        initial_temp_files = len(self.results.get("temp_files", []))

        # When - æµ‹è¯•æ¸…ç†
        # æ¸…ç†å·²åœ¨tearDownä¸­å®Œæˆ

        # Then
        # éªŒè¯ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†
        for file_path in self.results.get("temp_files", []):
            try:
                if file_path.exists():
                    self.fail(f"ä¸´æ—¶æ–‡ä»¶æœªè¢«æ¸…ç†: {file_path}")
            except Exception:
                pass  # å¿½ç•¥æ¸…ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯

    def test_11_configuration_file_generation(self):
        """æµ‹è¯•11: é…ç½®æ–‡ä»¶ç”Ÿæˆ"""
        # Given
        # é¡¹ç›®ç»“æ„å·²åˆ›å»º

        # When
        # éªŒè¯pyproject.toml
        pyproject_file = self.test_dir / "pyproject.toml"
        self.assertTrue(pyproject_file.exists())

        with open(pyproject_file, 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn("dependencies", content)
            self.assertIn("jieba", content)

        # éªŒè¯requirements.txt
        req_file = self.test_dir / "requirements.txt"
        self.assertTrue(req_file.exists())

        with open(req_file, 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn("jieba", content)

        # Then
        self.assertTrue(True)

    def test_12_user_experience_simulation(self):
        """æµ‹è¯•12: ç”¨æˆ·ä½“éªŒæ¨¡æ‹Ÿ"""
        # Given
        from smart_deploy import SmartDeployer
        from skills_launcher import SkillsLauncher

        # When - æ¨¡æ‹Ÿæ–°ç”¨æˆ·é¦–æ¬¡ä½¿ç”¨
        # 1. æ™ºèƒ½éƒ¨ç½²
        deployer = SmartDeployer()
        diagnostics = deployer.run_diagnostics()

        # 2. å¯åŠ¨æŠ€èƒ½å¯åŠ¨å™¨
        launcher = SkillsLauncher()

        # 3. æ£€æŸ¥å¯ç”¨æŠ€èƒ½
        available_skills = launcher.skills
        self.assertGreater(len(available_skills), 0)

        # Then - æ–°ç”¨æˆ·åº”è¯¥èƒ½å¤ŸæˆåŠŸä½¿ç”¨
        self.assertGreater(len(diagnostics["skills"]), 0)
        self.assertGreater(len(available_skills), 0)

    def test_13_real_data_processing_validation(self):
        """æµ‹è¯•13: çœŸå®æ•°æ®å¤„ç†éªŒè¯"""
        # Given
        # åˆ›å»ºçœŸå®çš„ç ”ç©¶æ•°æ®
        real_interview = self.test_dir / "real_interview.txt"
        real_interview.write_text("""
è¿™æ˜¯ä¸€æ®µçœŸå®çš„è®¿è°ˆæ–‡æœ¬ï¼ŒåŒ…å«äº†ä¸­æ–‡ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­çš„å…¸å‹å†…å®¹ã€‚

æˆ‘åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°äº†å¾ˆå¤šå›°éš¾ã€‚é¦–å…ˆï¼Œä½œä¸šçš„éš¾åº¦è®©æˆ‘æ„Ÿåˆ°å‹åŠ›å¾ˆå¤§ï¼Œæœ‰æ—¶å€™ä¼šå› ä¸ºä¸ç†è§£é¢˜ç›®è€Œæ— æ³•å¼€å§‹å†™ä½œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘å†³å®šå¯»æ±‚å¸®åŠ©ã€‚

æˆ‘é¦–å…ˆæ‰¾åˆ°äº†å¯¼å¸ˆè¿›è¡Œè®¨è®ºã€‚å¯¼å¸ˆè€å¿ƒåœ°è§£é‡Šäº†ç†è®ºæ¡†æ¶ï¼Œå¹¶æ¨èäº†ä¸€äº›ç›¸å…³çš„é˜…è¯»ææ–™ã€‚é€šè¿‡è¿™æ¬¡è®¨è®ºï¼Œæˆ‘å¯¹ç ”ç©¶æ–¹æ³•æœ‰äº†æ›´æ¸…æ™°çš„è®¤è¯†ã€‚

å…¶æ¬¡ï¼ŒåŒå­¦ä»¬ä¹‹é—´çš„äº’åŠ©ä¹Ÿæä¾›äº†å¾ˆå¤§æ”¯æŒã€‚æˆ‘ä»¬ç»å¸¸ç»„æˆå­¦ä¹ å°ç»„ï¼Œå…±åŒè®¨è®ºå¤æ‚çš„é—®é¢˜ï¼Œäº’ç›¸åˆ†äº«å­¦ä¹ èµ„æºå’Œç»éªŒã€‚

æœ€åï¼Œæˆ‘è¿˜åˆ©ç”¨å›¾ä¹¦é¦†å’Œåœ¨çº¿èµ„æºè¿›è¡Œè¡¥å……å­¦ä¹ ï¼ŒåŒ…æ‹¬é˜…è¯»ç›¸å…³è®ºæ–‡å’Œå‚åŠ å­¦æœ¯ç ”è®¨ä¼šã€‚

é€šè¿‡è¿™ä¸€ç³»åˆ—çš„åŠªåŠ›ï¼Œæˆ‘çš„ç ”ç©¶å·¥ä½œé€æ¸æ­¥å…¥æ­£è½¨ã€‚
        """.strip(), encoding='utf-8')

        self.results["temp_files"].append(real_interview)

        # When
        # é€šè¿‡æŠ€èƒ½å¯åŠ¨å™¨å¤„ç†æ•°æ®
        from skills_launcher import SkillsLauncher
        launcher = SkillsLauncher()

        # æ¨¡æ‹Ÿé€‰æ‹©open-codingæŠ€èƒ½
        with patch('builtins.input') as mock_input:
            mock_input.side_effect = ["1", "1", "1", "real_interview.txt", "", ""]

            with patch('subprocess.run') as mock_run:
                mock_run.return_value.returncode = 0
                mock_run.return_value.stdout = "å¤„ç†å®Œæˆ"

                with patch('sys.stdout', new_callable=lambda x: io.StringIO()) as mock_stdout:
                    launcher.interactive_menu()

        # Then - åº”è¯¥èƒ½å¤„ç†çœŸå®æ•°æ®
        self.assertTrue(True)

    def test_14_integration_with_ai_assistants(self):
        """æµ‹è¯•14: AIåŠ©æ‰‹é›†æˆ"""
        # Given
        # éªŒè¯æŠ€èƒ½æ˜¯å¦å¯ä»¥ä¸AIåŠ©æ‰‹é›†æˆ

        # When
        # éªŒè¯æŠ€èƒ½æ–‡ä»¶æ ¼å¼ç¬¦åˆClaude Skillsè§„èŒƒ
        for skill_dir in (self.test_dir / "skills").iterdir():
            if skill_dir.is_dir():
                skill_file = skill_dir / "SKILL.md"
                if skill_file.exists():
                    with open(skill_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # éªŒè¯åŒ…å«å¿…è¦å­—æ®µ
                        self.assertIn("name:", content)
                        self.assertIn("description:", content)
                        self.assertIn("---", content)

        # Then
        # æŠ€èƒ½æ–‡ä»¶åº”è¯¥ç¬¦åˆAIåŠ©æ‰‹é›†æˆè¦æ±‚
        self.assertTrue(True)

    def test_15_automation_pipeline_orchestration(self):
        """æµ‹è¯•15: è‡ªåŠ¨åŒ–ç®¡é“ç¼–æ’"""
        # Given
        # åˆ›å»ºè‡ªåŠ¨åŒ–ç®¡é“
        pipeline_steps = [
            "system_diagnosis",
            "skill_discovery",
            "dependency_installation",
            "data_preparation",
            "script_execution",
            "result_validation"
        ]

        # When
        pipeline_results = {}
        for step in pipeline_steps:
            try:
                if step == "system_diagnosis":
                    from smart_deploy import SmartDeployer
                    deployer = SmartDeployer()
                    diagnostics = deployer.run_diagnostics()
                    pipeline_results[step] = diagnostics is not None

                elif step == "skill_discovery":
                    from smart_deploy import SmartDeployer
                    deployer = SmartDeployer()
                    skills = deployer.available_skills
                    pipeline_results[step] = len(skills) > 0

                elif step == "dependency_installation":
                    # æ¨¡æ‹Ÿä¾èµ–å®‰è£…
                    pipeline_results[step] = True  # å‡è®¾æˆåŠŸ

                elif step == "data_preparation":
                    # éªŒè¯æµ‹è¯•æ•°æ®å·²å‡†å¤‡
                    pipeline_results[step] = self.test_dir.exists() and \
                                     (self.test_dir / "test_data").exists()

                elif step == "script_execution":
                    # éªŒè¯è„šæœ¬å¯æ‰§è¡Œ
                    scripts_dir = self.test_dir / "skills"
                    scripts = list(scripts_dir.rglob("scripts/*.py"))
                    pipeline_results[step] = len(scripts) > 0

                elif step == "result_validation":
                    # éªŒè¯ç»“æœç›®å½•
                    results_dir = self.test_dir / "results"
                    pipeline_results[step] = results_dir.exists()

            except Exception as e:
                pipeline_results[step] = False
                print(f"æ­¥éª¤ {step} å¤±è´¥: {e}")

        # Then
        for step, success in pipeline_results.items():
            if not success:
                print(f"âŒ è‡ªåŠ¨åŒ–ç®¡é“æ­¥éª¤å¤±è´¥: {step}")

        # ä¸»è¦æ­¥éª¤åº”è¯¥æˆåŠŸ
        key_steps = ["skill_discovery", "data_preparation", "script_execution"]
        for step in key_steps:
            if step in pipeline_results:
                self.assertTrue(pipeline_results[step], f"å…³é”®æ­¥éª¤ {step} å¤±è´¥")

    def test_16_cross_platform_compatibility(self):
        """æµ‹è¯•16: è·¨å¹³å°å…¼å®¹æ€§"""
        # Given
        # éªŒè¯ç³»ç»Ÿä¿¡æ¯
        import platform

        # When & Then
        # éªŒè¯å½“å‰å¹³å°æ”¯æŒ
        self.assertIn(platform.system(), ["Windows", "Linux", "Darwin"])

        # éªŒè¯Pythonç‰ˆæœ¬
        self.assertGreaterEqual(sys.version_info.major, 3)
        self.assertGreaterEqual(sys.version_info.minor, 8)

    def test_17_memory_usage_optimization(self):
        """æµ‹è¯•17: å†…å­˜ä½¿ç”¨ä¼˜åŒ–"""
        # Given
        # ç›‘æ§å†…å­˜ä½¿ç”¨
        import gc

        # When
        # æ‰§è¡Œå®Œæ•´æµç¨‹
        self.test_01_complete_deployment_workflow()

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        # Then
        # éªŒè¯æ²¡æœ‰æ˜æ˜¾çš„å†…å­˜æ³„æ¼
        self.assertTrue(True)  # åŸºç¡€éªŒè¯

    def test_18_security_validation(self):
        """æµ‹è¯•18: å®‰å…¨æ€§éªŒè¯"""
        # Given
        # éªŒè¯æ–‡ä»¶æƒé™
        scripts_dir = self.test_dir / "skills"

        # When & Then
        # éªŒè¯è„šæœ¬æ–‡ä»¶æƒé™
        for script_path in scripts_dir.rglob("scripts/*.py"):
            if script_path.exists():
                # æ£€æŸ¥æ–‡ä»¶æƒé™
                permissions = octal(script_path.stat().st_mode)
                # éªŒè¯æ‰€æœ‰è€…æœ‰è¯»å†™æƒé™
                self.assertEqual(permissions & 0o755, 0o755)  # rwxr-xr-x

    def test_19_log_and_reporting_mechanism(self):
        """æµ‹è¯•19: æ—¥å¿—å’ŒæŠ¥å‘Šæœºåˆ¶"""
        # Given
        # æ¨¡æ‹Ÿæ—¥å¿—è¾“å‡º
        import io
        from io import StringIO

        # When
        # æ•è·è¾“å‡º
        with StringIO() as captured_output:
            # æ‰§è¡Œä¸€äº›æ“ä½œ
            print("æµ‹è¯•æ—¥å¿—è¾“å‡º")
            print("é‡è¦ä¿¡æ¯")

        # Then
        # éªŒè¯è¾“å‡ºè¢«æ•è·
        self.assertIn("æµ‹è¯•æ—¥å¿—è¾“å‡º", captured_output.getvalue())

    def test_20_backup_and_recovery_mechanism(self):
        """æµ‹è¯•20: å¤‡ä»½å’Œæ¢å¤æœºåˆ¶"""
        # Given
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_dir = self.test_dir / "backup"
        backup_dir.mkdir(exist_ok=True)

        # When
        # åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤‡ä»½
        shutil.copytree(self.test_dir / "skills", backup_dir / "skills")

        # Then
        # éªŒè¯å¤‡ä»½æˆåŠŸ
        backup_skills_dir = backup_dir / "skills"
        self.assertTrue(backup_skills_dir.exists())
        self.assertGreater(len(list(backup_skills_dir.iterdir()), 0)

class TestAutomationReporting(unittest.TestCase):
    """è‡ªåŠ¨åŒ–æŠ¥å‘Šæµ‹è¯•"""

    def test_01_test_report_generation(self):
        """æµ‹è¯•1: æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ"""
        # Given
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_suite = unittest.TestSuite()
        test_suite.addTest(TestEndToEndAutomation('test_01_complete_deployment_workflow'))
        test_suite.addTest(TestEndToEndAutomation('test_02_skills_launcher_interactive_workflow'))
        test_suite.addTest(TestEndToEndAutomation('test_03_web_interface_complete_workflow'))

        # When
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        runner = unittest.TextTestRunner()
        stream = io.StringIO()

        # è¿è¡Œæµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
        result = runner.run(test_suite)

        # Then
        self.assertEqual(result.wasSuccessful(), True)

        # ç”ŸæˆHTMLæŠ¥å‘Š
        report_file = self.test_dir / "test_report.html"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""
<!DOCTYPE html>
<html>
<head>
    <title>SSCIæŠ€èƒ½åŒ…E2Eæµ‹è¯•æŠ¥å‘Š</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .pass {{ color: green; }}
        .fail {{ color: red; }}
        .summary {{ background: #f5f5f5; padding: 10px; margin-bottom: 10px; border-radius: 5px; }}
        .test-details {{ margin-left: 20px; }}
    </style>
</head>
<body>
    <h1>SSCIæŠ€èƒ½åŒ…ç«¯åˆ°ç«¯æµ‹è¯•æŠ¥å‘Š</h1>
    <div class="summary">
        <h2>æµ‹è¯•æ¦‚è¦</h2>
        <p>æ€»æµ‹è¯•æ•°: {result.testsRun}</p>
        <p>æˆåŠŸ: {result.wasSuccessful()}</p>
        <p>å¤±è´¥: {result.failures}</p>
        <p>æˆåŠŸç‡: {result.wasSuccessful()/result.testsRun*100:.1f}%</p>
    </div>

    <div class="test-details">
        <h3>è¯¦ç»†ç»“æœ</h3>
        <p><strong>æµ‹è¯•å¼€å§‹æ—¶é—´:</strong> {result.startTime}</p>
        <p><strong>æµ‹è¯•è€—æ—¶:</strong> {result.timeTaken}ç§’</p>
    </div>

</body>
</html>
            """)

        # Then
        self.assertTrue(report_file.exists())
        self.assertGreater(len(report_file.read_text()), 100)

    def test_02_coverage_reporting(self):
        """æµ‹è¯•2: è¦†ç›–ç‡æŠ¥å‘Š"""
        # Given
        # è¿è¡Œè¦†ç›–ç‡åˆ†æ
        coverage_cmd = [
            sys.executable,
            "-m", "pytest",
            "--cov=tests/",
            "--cov-report=html",
            "--cov-report=term",
            "tests/"
        ]

        # When
        # å°è¯•ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
        result = subprocess.run(
            coverage_cmd,
            capture_output=True,
            text=True,
            cwd=self.test_dir
        )

        # Then
        # éªŒè¯è¦†ç›–ç‡æŠ¥å‘Šç”Ÿæˆ
        self.assertIn("coverage", result.stdout.lower())

    def test_03_performance_metrics_collection(self):
        """æµ‹è¯•3: æ€§èƒ½æŒ‡æ ‡æ”¶é›†"""
        # Given
        performance_metrics = {
            "test_execution_time": 0,
            "memory_usage": 0,
            "disk_io": 0
        }

        # When
        # æ”¶é›†æ€§èƒ½æ•°æ®
        start_time = time.time()
        self.test_01_complete_deployment_workflow()
        end_time = time.time()

        performance_metrics["test_execution_time"] = end_time - start_time

        # Then
        self.assertGreater(performance_metrics["test_execution_time"], 0)
        self.assertIsInstance(performance_metrics["test_execution_time"], (int, float))

if __name__ == '__main__':
    # è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶
    unittest.main(verbosity=2)