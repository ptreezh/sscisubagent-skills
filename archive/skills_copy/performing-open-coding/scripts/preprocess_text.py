#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬é¢„å¤„ç†è„šæœ¬ - å¼€æ”¾ç¼–ç 

åŠŸèƒ½ï¼š
- ä¸­æ–‡åˆ†è¯ï¼ˆjiebaï¼‰
- åœç”¨è¯è¿‡æ»¤
- è¯­ä¹‰åˆ†æ®µ

ä½¿ç”¨æ–¹å¼ï¼š
  python preprocess_text.py --input raw.txt --output clean.json
"""

import argparse
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import jieba
import jieba.posseg as pseg

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

# åœç”¨è¯åˆ—è¡¨
STOPWORDS = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
    'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
    'è€Œ', 'ç€', 'ä½ ', 'ä¼š', 'çœ‹', 'èƒ½', 'ä¸‹', 'å¯¹', 'è¿™', 'æ¥',
    'ä»–', 'æ—¶', 'åœ°', 'ä»¬', 'å‡º', 'äº', 'ä¸º', 'å­', 'ä¸­', 'ä¸'
}

def tokenize_chinese(text: str, keep_pos: List[str] = None) -> List[str]:
    """
    ä¸­æ–‡åˆ†è¯
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        keep_pos: ä¿ç•™çš„è¯æ€§åˆ—è¡¨ï¼ˆé»˜è®¤ï¼šåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ï¼‰
    
    Returns:
        åˆ†è¯ç»“æœåˆ—è¡¨
    """
    if keep_pos is None:
        keep_pos = ['n', 'v', 'a', 'vn', 'an']  # åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯
    
    words = pseg.cut(text)
    result = []
    
    for word, flag in words:
        # ä¿ç•™æŒ‡å®šè¯æ€§ä¸”é•¿åº¦>1çš„è¯
        if any(flag.startswith(pos) for pos in keep_pos) and len(word) > 1:
            result.append(word)
    
    return result

def remove_stopwords(words: List[str], custom_stopwords: set = None) -> List[str]:
    """
    ç§»é™¤åœç”¨è¯
    
    Args:
        words: åˆ†è¯åˆ—è¡¨
        custom_stopwords: è‡ªå®šä¹‰åœç”¨è¯é›†åˆ
    
    Returns:
        è¿‡æ»¤åçš„è¯åˆ—è¡¨
    """
    stopwords = STOPWORDS.copy()
    if custom_stopwords:
        stopwords.update(custom_stopwords)
    
    return [w for w in words if w not in stopwords]

def segment_by_meaning(text: str, max_length: int = 500) -> List[str]:
    """
    æŒ‰è¯­ä¹‰åˆ†æ®µ
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        max_length: æ¯æ®µæœ€å¤§é•¿åº¦
    
    Returns:
        åˆ†æ®µåˆ—è¡¨
    """
    # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å¥
    import re
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
    
    segments = []
    current_segment = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        if len(current_segment) + len(sentence) <= max_length:
            current_segment += sentence + "ã€‚"
        else:
            if current_segment:
                segments.append(current_segment)
            current_segment = sentence + "ã€‚"
    
    if current_segment:
        segments.append(current_segment)
    
    return segments

def preprocess_text(text: str) -> Dict:
    """
    å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†æµç¨‹
    
    Returns:
        é¢„å¤„ç†ç»“æœå­—å…¸
    """
    # 1. åˆ†æ®µ
    segments = segment_by_meaning(text)
    
    # 2. åˆ†è¯
    all_words = []
    segment_words = []
    
    for seg in segments:
        words = tokenize_chinese(seg)
        words_filtered = remove_stopwords(words)
        all_words.extend(words_filtered)
        segment_words.append({
            'text': seg,
            'words': words_filtered,
            'word_count': len(words_filtered)
        })
    
    # 3. ç»Ÿè®¡
    from collections import Counter
    word_freq = Counter(all_words)
    
    return {
        'segments': segment_words,
        'total_words': len(all_words),
        'unique_words': len(set(all_words)),
        'word_frequency': dict(word_freq.most_common(50))
    }

def main():
    parser = argparse.ArgumentParser(
        description='æ–‡æœ¬é¢„å¤„ç†å·¥å…· - å¼€æ”¾ç¼–ç ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python preprocess_text.py --input interview.txt --output clean.json
  python preprocess_text.py -i raw.txt -o processed.json --max-length 300
        """
    )
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥æ–‡æœ¬æ–‡ä»¶')
    parser.add_argument('--output', '-o', default='preprocessed.json', help='è¾“å‡ºJSONæ–‡ä»¶')
    parser.add_argument('--max-length', type=int, default=500, help='æ¯æ®µæœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ï¼š500ï¼‰')
    args = parser.parse_args()
    
    start_time = time.time()
    
    try:
        # è¯»å–æ–‡ä»¶
        input_path = Path(args.input)
        if not input_path.exists():
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            sys.exit(1)
        
        with open(input_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        if not text.strip():
            logging.error("è¾“å…¥æ–‡ä»¶ä¸ºç©º")
            sys.exit(2)
        
        logging.info(f"âœ“ è¯»å–æ–‡ä»¶: {args.input} ({len(text)} å­—ç¬¦)")
        
        # é¢„å¤„ç†
        result = preprocess_text(text)
        processing_time = time.time() - start_time
        
        # æ„å»ºè¾“å‡º
        output = {
            'summary': {
                'total_segments': len(result['segments']),
                'total_words': result['total_words'],
                'unique_words': result['unique_words'],
                'processing_time': round(processing_time, 2)
            },
            'details': result,
            'metadata': {
                'input_file': str(input_path.absolute()),
                'timestamp': datetime.now().isoformat(),
                'version': '1.0.0'
            }
        }
        
        # ä¿å­˜
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        logging.info(f"âœ… é¢„å¤„ç†å®Œæˆ")
        logging.info(f"   åˆ†æ®µæ•°: {len(result['segments'])}")
        logging.info(f"   æ€»è¯æ•°: {result['total_words']}")
        logging.info(f"   ç‹¬ç‰¹è¯: {result['unique_words']}")
        logging.info(f"ğŸ“„ è¯¦ç»†ç»“æœ: {args.output}")
        
    except Exception as e:
        logging.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(99)

if __name__ == "__main__":
    main()
