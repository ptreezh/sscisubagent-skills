---
name: assessing-validity-reliability
description: è¯„ä¼°ç ”ç©¶ä¿¡åº¦å’Œæ•ˆåº¦ï¼ŒåŒ…æ‹¬å†…åœ¨æ•ˆåº¦ã€å¤–åœ¨æ•ˆåº¦ã€æ„å¿µæ•ˆåº¦ã€ç»Ÿè®¡ç»“è®ºæ•ˆåº¦çš„æ£€éªŒå’Œæå‡ã€‚å½“éœ€è¦éªŒè¯ç ”ç©¶è´¨é‡ã€å¤„ç†æ•ˆåº¦é—®é¢˜æˆ–æé«˜ç ”ç©¶ä¸¥è°¨æ€§æ—¶ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚
---

# ä¿¡åº¦æ•ˆåº¦åˆ†ææŠ€èƒ½

## ğŸ¯ æ ¸å¿ƒç›®æ ‡ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
ä¸ºä¸­æ–‡ç¤¾ä¼šç§‘å­¦ç ”ç©¶æä¾›ç§‘å­¦ã€ç³»ç»Ÿçš„ä¿¡åº¦æ•ˆåº¦è¯„ä¼°å’Œæå‡æ–¹æ¡ˆï¼Œç¡®ä¿ç ”ç©¶çš„ç§‘å­¦æ€§å’Œå¯ä¿¡åº¦ã€‚

## ğŸ“‹ å¿…é¡»é¦–å…ˆæŒæ¡çš„æ•ˆåº¦ç±»å‹

### 1. å››å¤§æ•ˆåº¦ç±»å‹
**æœ€é‡è¦æ¦‚å¿µ**ï¼š
- **å†…åœ¨æ•ˆåº¦**ï¼šå› æœæ¨æ–­çš„å‡†ç¡®æ€§ï¼Œç ”ç©¶æ˜¯å¦çœŸæ­£æµ‹é‡äº†æƒ³è¦æµ‹é‡çš„æ¦‚å¿µ
- **å¤–åœ¨æ•ˆåº¦**ï¼šç»“æœæ¨å¹¿çš„é€‚ç”¨æ€§ï¼Œç ”ç©¶ç»“æœæ˜¯å¦é€‚ç”¨äºå…¶ä»–æƒ…å¢ƒ
- **æ„å¿µæ•ˆåº¦**ï¼šæµ‹é‡çš„å‡†ç¡®æ€§ï¼Œæµ‹é‡å·¥å…·æ˜¯å¦çœŸæ­£åæ˜ äº†ç†è®ºæ¦‚å¿µ
- **ç»Ÿè®¡ç»“è®ºæ•ˆåº¦**ï¼šç»Ÿè®¡æ¨æ–­çš„å¯é æ€§ï¼Œç»Ÿè®¡ç»“è®ºæ˜¯å¦å‡†ç¡®

### 2. ä¸­æ–‡ç¤¾ç§‘ç ”ç©¶ç‰¹æ®ŠæŒ‘æˆ˜
**å¿…é¡»è€ƒè™‘**ï¼š
- **æ–‡åŒ–æ•æ„Ÿæ€§**ï¼šä¸­å›½æ–‡åŒ–çš„ç‰¹æ®Šæ€§å¯¹æµ‹é‡å·¥å…·çš„å½±å“
- **è¯­è¨€é€‚åˆ‡æ€§**ï¼šä¸­æ–‡è¡¨è¾¾å’Œç†è§£çš„å¤æ‚æ€§
- **åˆ¶åº¦ç¯å¢ƒ**ï¼šä¸­å›½åˆ¶åº¦ç¯å¢ƒå¯¹ç ”ç©¶ç»“æœçš„é™åˆ¶
- **å®è·µç›¸å…³æ€§**ï¼šç†è®ºä¸å®è·µçš„ç»“åˆåº¦

### 3. è´¨é‡æ§åˆ¶çš„é»„é‡‘æ ‡å‡†
**å¿…é¡»ç¡®ä¿**ï¼š
- **å¤šé‡éªŒè¯**ï¼šä½¿ç”¨å¤šç§æ–¹æ³•éªŒè¯åŒä¸€ç»“æœ
- **ä¸‰è§’éªŒè¯**ï¼šå®šé‡ã€å®šæ€§ã€æ–¹æ³•ä¸‰è§’éªŒè¯
- **ä¸“å®¶è¯„å®¡**ï¼šé¢†åŸŸä¸“å®¶çš„ç‹¬ç«‹è¯„ä¼°
- **é€æ˜æŠ¥å‘Š**ï¼šè¯¦ç»†æŠ¥å‘Šæ•ˆåº¦è¯„ä¼°è¿‡ç¨‹

## ğŸ”„ åŠ¨æ€çŸ¥è¯†åº“åŠ è½½

### å¯åŠ¨æ—¶åŠ è½½
```
/knowledge-base/main-knowledge.md
/knowledge-base/core-concepts.md
/knowledge-base/validity-fundamentals.md
```

### æŒ‰éœ€åŠ è½½
```
ç”¨æˆ·æ£€éªŒå†…åœ¨æ•ˆåº¦ â†’ /knowledge-base/internal-validity.md
ç”¨æˆ·æ£€éªŒå¤–åœ¨æ•ˆåº¦ â†’ /knowledge-base/external-validity.md
ç”¨æˆ·éœ€è¦æ•ˆåº¦æå‡ â†’ /knowledge-base/validity-improvement.md
```

## ğŸš¨ ç´§æ€¥å¤„ç†åè®®

### çº¢è‰²è­¦æŠ¥ï¼ˆè®ºæ–‡ç­”è¾©ï¼‰
**å¿«é€Ÿè¯„ä¼°æ¨¡å¼**ï¼š
1. å¿«é€Ÿæ£€æŸ¥å››å¤§æ•ˆåº¦åŸºæœ¬è¦æ±‚
2. è¯†åˆ«æœ€ä¸¥é‡çš„æ•ˆåº¦å¨èƒ
3. æä¾›ç´§æ€¥æ”¹è¿›æ–¹æ¡ˆ
4. ç”Ÿæˆæ•ˆåº¦æ£€æŸ¥æ¸…å•
5. æ‰¿è¯º48å°æ—¶å†…å®Œæˆè¯¦ç»†è¯„ä¼°

### é»„è‰²è­¦æŠ¥ï¼ˆå¯¼å¸ˆè¦æ±‚ï¼‰
**æ ‡å‡†è¯„ä¼°æ¨¡å¼**ï¼š
1. ç³»ç»Ÿæ€§å››å¤§æ•ˆåº¦æ£€éªŒ
2. è¯¦ç»†åˆ†ææ•ˆåº¦å¨èƒ
3. åˆ¶å®šæ”¹è¿›ç­–ç•¥
4. æä¾›æ•ˆåº¦æå‡æ–¹æ¡ˆ
5. ç”Ÿæˆå®Œæ•´è¯„ä¼°æŠ¥å‘Š

## ğŸ› ï¸ æ ¸å¿ƒè¯„ä¼°æŠ€èƒ½

### 1. å†…åœ¨æ•ˆåº¦è¯„ä¼°
**æ ¸å¿ƒæŠ€èƒ½**ï¼š
```python
def assess_internal_validity(research_design, data):
    """è¯„ä¼°å†…åœ¨æ•ˆåº¦"""
    validity_threats = {
        'history': {
            'threat': 'å†å²äº‹ä»¶å½±å“',
            'mitigation': ['å‰æµ‹-åæµ‹è®¾è®¡', 'å¯¹ç…§ç»„è®¾è®¡', 'ç»Ÿè®¡æ§åˆ¶'],
            'assessment': check_history_threats
        },
        'maturation': {
            'threat': 'è¢«è¯•è€…æˆç†Ÿå˜åŒ–',
            'mitigation': ['ç¼©çŸ­ç ”ç©¶å‘¨æœŸ', 'æˆç†Ÿåº¦åŒ¹é…', 'ç»Ÿè®¡åˆ†æ'],
            'assessment': check_maturation_threats
        },
        'testing': {
            'threat': 'æµ‹è¯•æ•ˆåº”å½±å“',
            'mitigation': ['Solomonå››ç»„è®¾è®¡', 'éšè”½æµ‹è¯•', 'å·¥å…·å¹³è¡¡'],
            'assessment': check_testing_threats
        },
        'instrumentation': {
            'threat': 'æµ‹é‡å·¥å…·ä¸å‡†ç¡®',
            'mitigation': 'å·¥å…·éªŒè¯', 'é¢„æµ‹è¯•', 'æ ¡å‡†ç¨‹åº'],
            'assessment': check_instrumentation_threats
        }
    }
    
    # ç»¼åˆè¯„ä¼°
    overall_validity = calculate_overall_validity(validity_threats, data)
    
    return {
        'threats': validity_threats,
        'overall_score': overall_validity,
        'recommendations': generate_improvement_recommendations(validity_threats),
        'timeline': create_improvement_timeline(overall_validity)
    }

def check_history_threats(research_design, data):
    """æ£€æŸ¥å†å²å¨èƒ"""
    threats = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤§äº‹ä»¶
    major_events = identify_major_events(research_design['timeframe'])
    if major_events:
        threats.append({
            'type': 'å†å²äº‹ä»¶',
            'event': major_events,
            'impact': 'å¯èƒ½å½±å“ç ”ç©¶ç»“æœ',
            'mitigation': 'éœ€è¦åœ¨åˆ†æä¸­æ§åˆ¶'
        })
    
    return threats

def calculate_overall_validity(threats, data):
    """è®¡ç®—ç»¼åˆæ•ˆåº¦"""
    threat_scores = {
        'history': 0.8,
        'maturation': 0.7,
        'testing': 0.6,
        'instrumentation': 0.9
    }
    
    total_threat = 0
    weighted_threat = 0
    
    for threat_type, threats_list in threats.items():
        for threat in threats_list:
            total_threat += 1
            weighted_threat += threat_scores[threat_type]
    
    if total_threat == 0:
        return 1.0
    
    return 1.0 - (weighted_threat / total_threat)
```

### 2. å¤–åœ¨æ•ˆåº¦è¯„ä¼°
**æ ¸å¿ƒæŠ€èƒ½**ï¼š
```python
def assess_external_validity(research_results, target_population):
    """è¯„ä¼°å¤–åœ¨æ•ˆåº¦"""
    validity_types = {
        'population_validity': {
            'description': 'ç›®æ ‡æ€»ä½“çš„ä»£è¡¨æ€§',
            'assessment': assess_population_representativeness
        },
        'ecological_validity': {
            'description': 'çœŸå®ç¯å¢ƒçš„é€‚ç”¨æ€§',
            'assessment': assess_ecological_applicability
        },
        'temporal_validity': {
            'description': 'æ—¶é—´è·¨åº¦çš„é€‚ç”¨æ€§',
            'assessment': assess_temporal_generalizability
        },
        'cross_cultural_validity': {
            'description': 'è·¨æ–‡åŒ–çš„é€‚ç”¨æ€§',
            'assessment': assess_cultural_transferability
        }
    }
    
    validity_scores = {}
    for validity_type, details in validity_types.items():
        validity_scores[validity_type] = details['assessment'](research_results, target_population)
    
    return {
        'scores': validity_scores,
        'overall_score': sum(validity_scores.values()) / len(validity_scores),
        'limitations': identify_external_limitations(validity_scores),
        'generalizability_statement': generate_generalizability_statement(validity_scores)
    }

def assess_population_representativeness(sample, population):
    """è¯„ä¼°æ ·æœ¬ä»£è¡¨æ€§"""
    representativeness_indicators = {
        'sample_size_adequacy': check_sample_size(sample, population),
        'sampling_method_appropriateness': check_sampling_method(sample),
        'demographic_similarity': check_demographic_similarity(sample, population),
        'geographic_coverage': check_geographic_coverage(sample, population)
    }
    
    # ç»¼åˆè¯„ä¼°
    adequacy_score = calculate_representativeness_score(representativeness_indicators)
    
    return adequacy_score
```

### 3. æ„å¿µæ•ˆåº¦è¯„ä¼°
**æ ¸å¿ƒæŠ€èƒ½**ï¼š
```python
def assess_construct_validity(measurement_tools, theoretical_concepts):
    """è¯„ä¼°æ„å¿µæ•ˆåº¦"""
    validity_assessment = {
        'content_validity': {
            'description': 'æµ‹é‡å†…å®¹çš„å…¨é¢æ€§',
            'assessment': assess_content_coverage
        },
        'criterion_validity': {
            'description': 'æµ‹é‡æ ‡å‡†çš„å‡†ç¡®æ€§',
            'assessment': assess_criterion_accuracy
        },
        'convergent_validity': {
            'description': 'å¤šç§æ–¹æ³•çš„ä¸€è‡´æ€§',
            'assessment': assess_convergent_methods
        },
        'discriminant_validity': {
            'description': 'åŒºåˆ†ä¸åŒæ¦‚å¿µçš„èƒ½åŠ›',
            'assessment': assess_discriminant_ability
        }
    }
    
    validity_scores = {}
    for validity_type, details in validity_assessment.items():
        validity_scores[validity_type] = details['assessment'](measurement_tools, theoretical_concepts)
    
    return {
        'scores': validity_scores,
        'overall_score': sum(validity_scores.values()) / len(validity_scores),
        'improvement_strategies': generate_construct_improvement_strategies(validity_scores),
        'validation_matrix': create_validation_matrix(measurement_tools, theoretical_concepts)
    }

def assess_content_coverage(measurement_items, theoretical_framework):
    """è¯„ä¼°å†…å®¹è¦†ç›–åº¦"""
    framework_concepts = extract_concepts(theoretical_framework)
    measured_concepts = extract_concepts(measurement_items)
    
    coverage_ratio = len(measured_concepts & framework_concepts) / len(framework_concepts)
    
    return coverage_ratio
```

### 4. ç»Ÿè®¡ç»“è®ºæ•ˆåº¦è¯„ä¼°
**æ ¸å¿ƒæŠ€èƒ½**ï¼š
```python
def assess_statistical_conclusion_validity(statistical_analysis):
    """è¯„ä¼°ç»Ÿè®¡ç»“è®ºæ•ˆåº¦"""
    validity_checks = {
        'sample_size_adequacy': check_sample_size(statistical_analysis),
        'assumption_violations': check_assumptions(statistical_analysis),
        'statistical_power': check_statistical_power(statistical_analysis),
        'multiple_comparisons': check_multiple_comparisons(statistical_analysis),
        'data_distribution': check_data_distribution(statistical_analysis)
    }
    
    validity_score = calculate_statistical_validity(validity_checks)
    
    return {
        'checks': validity_checks,
        'overall_score': validity_score,
        'threats': identify_statistical_threats(validity_checks),
        'recommendations': generate_statistical_recommendations(validity_checks)
    }

def check_statistical_power(statistical_analysis):
    """æ£€æŸ¥ç»Ÿè®¡åŠŸæ•ˆ"""
    effect_size = statistical_analysis.get('effect_size')
    sample_size = statistical_analysis.get('sample_size')
    alpha = statistical_analysis.get('alpha', 0.05)
    
    # ä½¿ç”¨Cohen's dè®¡ç®—åŠŸæ•ˆ
    if effect_size and sample_size:
        power = calculate_cohen_d_power(effect_size, sample_size, alpha)
        return power
    else:
        return None

def calculate_cohen_d_power(effect_size, sample_size, alpha):
    """è®¡ç®—Cohen's dçš„åŠŸæ•ˆ"""
    # ç®€åŒ–çš„åŠŸæ•ˆè®¡ç®—
    z_alpha = 1.96  # åŒå°¾æ£€éªŒï¼Œalpha=0.05
    z_beta = 0.84   # åŠŸï¿½æ•ˆ=0.80
    
    n = sample_size
    d = effect_size
    
    # Cohen's dåŠŸæ•ˆå…¬å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
    power = norm.cdf((n * d / 2) - z_alpha) + norm.cdf((-n * d / 2) - z_alpha)
    
    return power
```

## ğŸ“Š è´¨é‡æ£€æŸ¥æ¸…å•

### æ•ˆåº¦è¯„ä¼°å®Œæ•´æ€§
- [ ] æ˜¯å¦è¿›è¡Œäº†å››ç§æ•ˆåº¦è¯„ä¼°
- [ ] æ˜¯å¦è¯†åˆ«äº†ä¸»è¦æ•ˆåº¦å¨èƒ
- [ ] æ˜¯å¦æä¾›äº†æ”¹è¿›å»ºè®®
- [ ] æ˜¯å¦åˆ¶å®šäº†æ”¹è¿›è®¡åˆ’
- [ ] æ˜¯å¦è®°å½•äº†è¯„ä¼°è¿‡ç¨‹

### è¯„ä¼°æ–¹æ³•ç§‘å­¦æ€§
- [ ] è¯„ä¼°æ–¹æ³•æ˜¯å¦é€‚åˆç ”ç©¶ç±»å‹
- [ ] è¯„ä¼°å·¥å…·æ˜¯å¦å¯é æœ‰æ•ˆ
- [ ] è¯„ä¼°è¿‡ç¨‹æ˜¯å¦é€æ˜å¯è¿½æº¯
- [ ] è¯„ä¼°ç»“æœæ˜¯å¦å®¢è§‚å‡†ç¡®
- [ ] æ”¹è¿›å»ºè®®æ˜¯å¦å…·ä½“å¯è¡Œ

### ä¸­æ–‡æœ¬åœŸåŒ–é€‚é…
- [ ] æ˜¯å¦è€ƒè™‘äº†ä¸­å›½æ–‡åŒ–ç‰¹æ®Šæ€§
- [ ] æ˜¯å¦é€‚é…äº†ä¸­æ–‡è¯­å¢ƒ
- [ ] æ˜¯å¦ç»“åˆäº†ä¸­å›½æ¡ˆä¾‹
- [ ] æ˜¯å¦ç¬¦åˆä¸­æ–‡å­¦æœ¯è§„èŒƒ
- [ ] æ˜¯å¦è€ƒè™‘äº†å®è·µç›¸å…³æ€§

## ğŸ’¡ å¿«é€Ÿå“åº”æ¨¡æ¿

### ç´§æ€¥æ•ˆåº¦è¯„ä¼°æ¨¡æ¿
```
1. å¿«é€Ÿæ£€æŸ¥å››å¤§æ•ˆåº¦åŸºæœ¬è¦æ±‚
2. è¯†åˆ«æœ€ä¸¥é‡çš„æ•ˆåº¦å¨èƒ
3. æä¾›ç´§æ€¥æ”¹è¿›æ–¹æ¡ˆ
4. ç”Ÿæˆæ•ˆåº¦æ£€æŸ¥æ¸…å•
5. æ‰¿è¯º48å°æ—¶å†…å®Œæˆè¯¦ç»†è¯„ä¼°
```

### æ ‡å‡†æ•ˆåº¦è¯„ä¼°æ¨¡æ¿
```
1. ç³»ç»Ÿæ€§å››å¤§æ•ˆåº¦æ£€éªŒ
2. è¯¦ç»†åˆ†ææ•ˆåº¦å¨èƒ
3. åˆ¶å®šæ”¹è¿›ç­–ç•¥
4. æä¾›æ•ˆåº¦æå‡æ–¹æ¡ˆ
5. ç”Ÿæˆå®Œæ•´è¯„ä¼°æŠ¥å‘Š
```

---

**ä½¿ç”¨è¯´æ˜**ï¼šæ­¤æŠ€èƒ½ä¸¥æ ¼éµå¾ªç¤¾ä¼šç§‘å­¦ç ”ç©¶æ•ˆåº¦è¯„ä¼°è§„èŒƒï¼Œç¡®ä¿ç ”ç©¶çš„ç§‘å­¦æ€§å’Œå¯ä¿¡åº¦ã€‚