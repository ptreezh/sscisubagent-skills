#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fsQCAæŠ€èƒ½å…¨é¢ç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰åŠŸèƒ½æ¨¡å—ã€å®šæ€§å®šé‡ç»“åˆæœºåˆ¶ã€æ¸è¿›å¼æŠ«éœ²ç­‰åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
import json
from typing import Dict, List, Tuple, Optional, Any

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'scripts'))

from calibration import FSCCalibration, consistency_xy, coverage_xy
from truth_table import FuzzyTruthTableBuilder
from minimization import FSCMinimizer


def test_calibration_module():
    """æµ‹è¯•æ ¡å‡†æ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ ¡å‡†æ¨¡å—...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    test_data = pd.Series(np.random.uniform(0, 10, 20), name='test_var')
    
    # åˆå§‹åŒ–æ ¡å‡†å™¨
    calibrator = FSCCalibration()
    
    # æµ‹è¯•æ‰€æœ‰æ ¡å‡†æ–¹æ³•
    methods = ['direct', 'threshold', 'interpolation', 'gaussian', 'sigmoid', 'indirect']
    results = {}
    
    for method in methods:
        try:
            if method == 'direct':
                calibrated = calibrator.calibrate_variable(test_data, method=method, thresholds=(8, 5, 2))
            elif method == 'threshold':
                calibrated = calibrator.calibrate_variable(test_data, method=method, thresholds=(7, 3))
            else:
                calibrated = calibrator.calibrate_variable(test_data, method=method)
            
            # éªŒè¯æ ¡å‡†ç»“æœ
            assert calibrated.min() >= 0, f"{method}æ ¡å‡†ç»“æœå°äº0"
            assert calibrated.max() <= 1, f"{method}æ ¡å‡†ç»“æœå¤§äº1"
            assert len(calibrated) == len(test_data), f"{method}æ ¡å‡†é•¿åº¦ä¸åŒ¹é…"
            
            results[method] = {
                'range': (calibrated.min(), calibrated.max()),
                'valid': True,
                'length': len(calibrated)
            }
            print(f"  âœ“ {method}æ ¡å‡†: èŒƒå›´ [{calibrated.min():.3f}, {calibrated.max():.3f}]")
        except Exception as e:
            results[method] = {'valid': False, 'error': str(e)}
            print(f"  âœ— {method}æ ¡å‡†: å¤±è´¥ - {str(e)}")
    
    # æµ‹è¯•ä¸€è‡´æ€§ã€è¦†ç›–åº¦è®¡ç®—å‡½æ•°
    try:
        test_x = pd.Series([0.8, 0.6, 0.9, 0.3, 0.7])
        test_y = pd.Series([0.9, 0.5, 0.7, 0.2, 0.8])
        
        consistency = consistency_xy(test_x, test_y)
        coverage = coverage_xy(test_x, test_y)
        
        assert 0 <= consistency <= 1, "ä¸€è‡´æ€§å€¼è¶…å‡ºèŒƒå›´"
        assert 0 <= coverage <= 1, "è¦†ç›–åº¦å€¼è¶…å‡ºèŒƒå›´"
        
        print(f"  âœ“ ä¸€è‡´æ€§è®¡ç®—: {consistency:.3f}")
        print(f"  âœ“ è¦†ç›–åº¦è®¡ç®—: {coverage:.3f}")
        results['consistency_coverage'] = {'valid': True, 'consistency': consistency, 'coverage': coverage}
    except Exception as e:
        print(f"  âœ— ä¸€è‡´æ€§/è¦†ç›–åº¦è®¡ç®—: å¤±è´¥ - {str(e)}")
        results['consistency_coverage'] = {'valid': False, 'error': str(e)}
    
    print("  æ ¡å‡†æ¨¡å—æµ‹è¯•å®Œæˆ\n")
    return results


def test_truth_table_module():
    """æµ‹è¯•çœŸå€¼è¡¨æ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•çœŸå€¼è¡¨æ¨¡å—...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    test_data = pd.DataFrame({
        'A': np.random.uniform(0, 1, 20),
        'B': np.random.uniform(0, 1, 20),
        'C': np.random.uniform(0, 1, 20),
        'outcome': np.random.uniform(0, 1, 20)
    })
    
    # åˆå§‹åŒ–çœŸå€¼è¡¨æ„å»ºå™¨
    tt_builder = FuzzyTruthTableBuilder()
    
    results = {}
    
    try:
        # æ„å»ºçœŸå€¼è¡¨
        truth_table = tt_builder.build_truth_table(
            test_data,
            ['A', 'B', 'C'],
            'outcome'
        )
        
        # éªŒè¯çœŸå€¼è¡¨ç»“æ„
        assert 'configuration' in truth_table.columns, "ç¼ºå°‘configurationåˆ—"
        assert 'consistency' in truth_table.columns, "ç¼ºå°‘consistencyåˆ—"
        assert 'outcome' in truth_table.columns, "ç¼ºå°‘outcomeåˆ—"
        
        print(f"  âœ“ çœŸå€¼è¡¨å½¢çŠ¶: {truth_table.shape}")
        print(f"  âœ“ çŸ›ç›¾ç»„åˆæ•°é‡: {len(tt_builder.contradictory_cases)}")
        print(f"  âœ“ é€»è¾‘ä½™é¡¹æ•°é‡: {len(tt_builder.logical_remainders)}")
        
        results['build'] = {
            'valid': True,
            'shape': truth_table.shape,
            'contradictory_count': len(tt_builder.contradictory_cases),
            'remainder_count': len(tt_builder.logical_remainders)
        }
        
        # æµ‹è¯•çŸ›ç›¾ç»„åˆå¤„ç†
        processed_table = tt_builder.handle_contradictions(method='remove')
        print(f"  âœ“ çŸ›ç›¾å¤„ç†åå½¢çŠ¶: {processed_table.shape}")
        
        results['contradiction_handling'] = {
            'valid': True,
            'processed_shape': processed_table.shape
        }
        
        # æµ‹è¯•è´¨é‡æŒ‡æ ‡è®¡ç®—
        quality_metrics = tt_builder.calculate_quality_metrics()
        print(f"  âœ“ è´¨é‡æŒ‡æ ‡: {quality_metrics}")
        
        results['quality_metrics'] = {
            'valid': True,
            'metrics': quality_metrics
        }
        
    except Exception as e:
        print(f"  âœ— çœŸå€¼è¡¨æ„å»º: å¤±è´¥ - {str(e)}")
        results['build'] = {'valid': False, 'error': str(e)}
    
    print("  çœŸå€¼è¡¨æ¨¡å—æµ‹è¯•å®Œæˆ\n")
    return results


def test_minimization_module():
    """æµ‹è¯•æœ€å°åŒ–æ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æœ€å°åŒ–æ¨¡å—...")
    
    # åˆ›å»ºæ¨¡æ‹ŸçœŸå€¼è¡¨æ•°æ®
    np.random.seed(42)
    
    # æ¨¡æ‹ŸçœŸå€¼è¡¨æ•°æ®
    sample_configs = [
        (1, 0, 1),  # é…ç½®1
        (1, 1, 0),  # é…ç½®2
        (0, 1, 1),  # é…ç½®3
        (1, 1, 1),  # é…ç½®4
    ]
    
    sample_data = []
    for i, config in enumerate(sample_configs):
        sample_data.append({
            'configuration': config,
            'frequency': np.random.randint(1, 5),
            'outcome': np.random.uniform(0.6, 1.0),  # æ­£é¢ç»“æœ
            'consistency': np.random.uniform(0.8, 1.0),
            'cases': [f'case_{i*3+j}' for j in range(np.random.randint(1, 4))],
            'n_cases': np.random.randint(1, 4),
            'inclusion_score': np.random.uniform(0.7, 1.0),
            'PRI_consistency': np.random.uniform(0.75, 1.0),
            'remainder': False,
            'contradictory': False
        })
    
    truth_table = pd.DataFrame(sample_data)
    
    # æ·»åŠ æ¡ä»¶åˆ—çš„å¹³å‡å€¼
    for i, condition in enumerate(['A', 'B', 'C']):
        truth_table[f'avg_{condition}'] = [config[i] for config in sample_configs]
    
    # åˆå§‹åŒ–æœ€å°åŒ–å™¨
    minimizer = FSCMinimizer()
    
    results = {}
    
    try:
        # æ‰§è¡Œæœ€å°åŒ–
        solutions = minimizer.minimize(truth_table, ['A', 'B', 'C'])
        
        print(f"  âœ“ ç”Ÿæˆè§£çš„æ•°é‡: {len(solutions)}")
        for i, solution in enumerate(solutions):
            print(f"    è§£ {i+1} ({solution.solution_type.value}): ä¸€è‡´æ€§={solution.consistency:.3f}, è¦†ç›–åº¦={solution.coverage:.3f}")
        
        results['minimization'] = {
            'valid': True,
            'solution_count': len(solutions),
            'solutions': [
                {
                    'type': solution.solution_type.value,
                    'expression': solution.expression,
                    'consistency': solution.consistency,
                    'coverage': solution.coverage,
                    'complexity': solution.complexity
                } for solution in solutions
            ]
        }
        
        # æµ‹è¯•è§£çš„è´¨é‡è¯„ä¼°
        if solutions:
            quality_metrics = minimizer.calculate_solution_quality(solutions[0], truth_table)
            print(f"  âœ“ è§£è´¨é‡æŒ‡æ ‡: {quality_metrics}")
            
            results['quality_assessment'] = {
                'valid': True,
                'metrics': quality_metrics
            }
            
            # æµ‹è¯•è§£çš„è¯„ä¼°
            evaluation = minimizer.evaluate_solution(solutions[0], truth_table, ['A', 'B', 'C'], 'outcome')
            print(f"  âœ“ è§£è¯„ä¼°: {evaluation['solution_type']}, è§£é‡Šæ€§={evaluation['interpretability']:.3f}, ç¨³å¥æ€§={evaluation['robustness']:.3f}")
            
            results['solution_evaluation'] = {
                'valid': True,
                'evaluation': evaluation
            }
        
    except Exception as e:
        print(f"  âœ— æœ€å°åŒ–æ‰§è¡Œ: å¤±è´¥ - {str(e)}")
        results['minimization'] = {'valid': False, 'error': str(e)}
    
    print("  æœ€å°åŒ–æ¨¡å—æµ‹è¯•å®Œæˆ\n")
    return results


def test_integration_module():
    """æµ‹è¯•é›†æˆåˆ†ææ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•é›†æˆåˆ†ææ¨¡å—...")
    
    try:
        # å¯¼å…¥é›†æˆåˆ†æå™¨
        from integrated_analysis import IntegratedFSCAnalyzer
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        np.random.seed(42)
        sample_data = pd.DataFrame({
            'case_id': range(1, 11),
            'A': np.random.uniform(0, 1, 10),
            'B': np.random.uniform(0, 1, 10),
            'C': np.random.uniform(0, 1, 10),
            'Y': np.random.uniform(0, 1, 10)
        })
        
        # åˆå§‹åŒ–åˆ†æå™¨
        skill_root = os.path.dirname(__file__)
        analyzer = IntegratedFSCAnalyzer(skill_root)
        
        # æµ‹è¯•å„é˜¶æ®µåŠŸèƒ½
        research_context = {
            'research_question': 'æµ‹è¯•ç ”ç©¶é—®é¢˜',
            'theoretical_framework': 'æµ‹è¯•æ¡†æ¶',
            'case_description': '10ä¸ªæµ‹è¯•æ¡ˆä¾‹'
        }
        
        # 1. ç†è®ºåˆ†æé˜¶æ®µ
        theoretical_analysis = analyzer.execute_theoretical_analysis(research_context)
        print("  âœ“ ç†è®ºåˆ†æé˜¶æ®µå®Œæˆ")
        
        # 2. æ ¡å‡†æŒ‡å¯¼é˜¶æ®µ
        calibration_plan = analyzer.execute_calibration_guidance(sample_data, theoretical_analysis)
        print("  âœ“ æ ¡å‡†æŒ‡å¯¼é˜¶æ®µå®Œæˆ")
        
        # 3. å®šé‡åˆ†æé˜¶æ®µ
        conditions = ['A', 'B', 'C']
        outcome = 'Y'
        quantitative_results = analyzer.execute_quantitative_analysis(
            sample_data, conditions, outcome, calibration_plan
        )
        print("  âœ“ å®šé‡åˆ†æé˜¶æ®µå®Œæˆ")
        
        # 4. ç»“æœè§£é‡Šé˜¶æ®µ
        interpretation = analyzer.execute_result_interpretation(
            quantitative_results, theoretical_analysis
        )
        print("  âœ“ ç»“æœè§£é‡Šé˜¶æ®µå®Œæˆ")
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_analysis_report("comprehensive_test_report.md")
        print("  âœ“ æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        results = {
            'theoretical_analysis': {'valid': True},
            'calibration_guidance': {'valid': True},
            'quantitative_analysis': {'valid': True},
            'result_interpretation': {'valid': True},
            'report_generation': {'valid': True},
            'integration_success': True
        }
        
        print(f"  âœ“ é›†æˆåˆ†ææµç¨‹å®Œæˆï¼Œå½“å‰é˜¶æ®µ: {analyzer.analysis_state['phase']}")
        
    except Exception as e:
        print(f"  âœ— é›†æˆåˆ†ææµç¨‹: å¤±è´¥ - {str(e)}")
        results = {
            'integration_success': False,
            'error': str(e)
        }
    
    print("  é›†æˆåˆ†ææ¨¡å—æµ‹è¯•å®Œæˆ\n")
    return results


def test_documentation_links():
    """æµ‹è¯•æ‰€æœ‰æ–‡æ¡£é“¾æ¥çš„æœ‰æ•ˆæ€§"""
    print("ğŸ§ª æµ‹è¯•æ–‡æ¡£é“¾æ¥...")
    
    skill_root = Path(__file__).parent
    results = {}
    
    # æµ‹è¯•æç¤ºè¯æ–‡ä»¶
    prompt_files = [
        'prompts/theoretical-analysis.md',
        'prompts/calibration-guidance.md',
        'prompts/result-interpretation.md',
        'prompts/theoretical-analysis-outline.md',
        'prompts/calibration-guidance-outline.md',
        'prompts/result-interpretation-outline.md'
    ]
    
    prompt_results = {}
    for file_path in prompt_files:
        full_path = skill_root / file_path
        exists = full_path.exists()
        prompt_results[file_path] = exists
        if exists:
            print(f"  âœ“ {file_path}")
        else:
            print(f"  âœ— {file_path} - ä¸å­˜åœ¨")
    
    results['prompts'] = prompt_results
    
    # æµ‹è¯•å‚è€ƒæ–‡æ¡£
    reference_files = [
        'references/METHODOLOGY.md',
        'references/BEST_PRACTICES.md',
        'references/METHODOLOGY-outline.md',
        'references/BEST_PRACTICES-outline.md'
    ]
    
    reference_results = {}
    for file_path in reference_files:
        full_path = skill_root / file_path
        exists = full_path.exists()
        reference_results[file_path] = exists
        if exists:
            print(f"  âœ“ {file_path}")
        else:
            print(f"  âœ— {file_path} - ä¸å­˜åœ¨")
    
    results['references'] = reference_results
    
    # æµ‹è¯•è„šæœ¬æ–‡ä»¶
    script_files = [
        'scripts/calibration.py',
        'scripts/truth_table.py',
        'scripts/minimization.py',
        'scripts/integrated_analysis.py'
    ]
    
    script_results = {}
    for file_path in script_files:
        full_path = skill_root / file_path
        exists = full_path.exists()
        script_results[file_path] = exists
        if exists:
            print(f"  âœ“ {file_path}")
        else:
            print(f"  âœ— {file_path} - ä¸å­˜åœ¨")
    
    results['scripts'] = script_results
    
    # æµ‹è¯•èµ„äº§æ–‡ä»¶
    asset_files = [
        'assets/templates/report_template.md'
    ]
    
    asset_results = {}
    for file_path in asset_files:
        full_path = skill_root / file_path
        exists = full_path.exists()
        asset_results[file_path] = exists
        if exists:
            print(f"  âœ“ {file_path}")
        else:
            print(f"  âœ— {file_path} - ä¸å­˜åœ¨")
    
    results['assets'] = asset_results
    
    print("  æ–‡æ¡£é“¾æ¥æµ‹è¯•å®Œæˆ\n")
    return results


def test_progressive_disclosure():
    """æµ‹è¯•æ¸è¿›å¼æŠ«éœ²åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ¸è¿›å¼æŠ«éœ²åŠŸèƒ½...")
    
    skill_root = Path(__file__).parent
    
    # è¯»å–SKILL.mdæ–‡ä»¶ï¼ŒéªŒè¯å…¶ç»“æ„
    skill_file = skill_root / 'SKILL.md'
    if not skill_file.exists():
        print("  âœ— SKILL.md æ–‡ä»¶ä¸å­˜åœ¨")
        return {'valid': False, 'error': 'SKILL.md not found'}
    
    with open(skill_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¸è¿›å¼æŠ«éœ²ç»“æ„
    has_level_1 = 'Level 1: æ ¸å¿ƒå…ƒæ•°æ®' in content
    has_level_2 = 'Level 2: æ“ä½œæ¡†æ¶' in content
    has_level_3 = 'Level 3: ä¸“ä¸šæç¤ºè¯' in content
    has_level_4 = 'Level 4: è®¡ç®—è„šæœ¬' in content
    
    print(f"  âœ“ Level 1 (æ ¸å¿ƒå…ƒæ•°æ®): {'âœ“' if has_level_1 else 'âœ—'}")
    print(f"  âœ“ Level 2 (æ“ä½œæ¡†æ¶): {'âœ“' if has_level_2 else 'âœ—'}")
    print(f"  âœ“ Level 3 (ä¸“ä¸šæç¤ºè¯): {'âœ“' if has_level_3 else 'âœ—'}")
    print(f"  âœ“ Level 4 (è®¡ç®—è„šæœ¬): {'âœ“' if has_level_4 else 'âœ—'}")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å®šæ€§å®šé‡ç»“åˆæœºåˆ¶
    has_qualitative_quantitative = 'å®šæ€§å®šé‡ç»“åˆæœºåˆ¶' in content
    print(f"  âœ“ å®šæ€§å®šé‡ç»“åˆæœºåˆ¶: {'âœ“' if has_qualitative_quantitative else 'âœ—'}")
    
    results = {
        'valid': all([has_level_1, has_level_2, has_level_3, has_level_4]),
        'levels': {
            'level_1': has_level_1,
            'level_2': has_level_2,
            'level_3': has_level_3,
            'level_4': has_level_4
        },
        'qualitative_quantitative': has_qualitative_quantitative
    }
    
    print("  æ¸è¿›å¼æŠ«éœ²åŠŸèƒ½æµ‹è¯•å®Œæˆ\n")
    return results


def test_qualitative_quantitative_integration():
    """æµ‹è¯•å®šæ€§å®šé‡ç»“åˆæœºåˆ¶"""
    print("ğŸ§ª æµ‹è¯•å®šæ€§å®šé‡ç»“åˆæœºåˆ¶...")
    
    # éªŒè¯é›†æˆåˆ†æå™¨å¦‚ä½•åè°ƒå®šæ€§ä¸å®šé‡
    try:
        from integrated_analysis import IntegratedFSCAnalyzer
        
        # æ£€æŸ¥ç±»çš„ç»“æ„
        analyzer = IntegratedFSCAnalyzer('.')
        
        # éªŒè¯æ˜¯å¦åŒ…å«å®šæ€§æ–¹æ³•
        has_theoretical_analysis = hasattr(analyzer, 'execute_theoretical_analysis')
        has_calibration_guidance = hasattr(analyzer, 'execute_calibration_guidance')
        has_result_interpretation = hasattr(analyzer, 'execute_result_interpretation')
        
        # éªŒè¯æ˜¯å¦åŒ…å«å®šé‡ç»„ä»¶
        has_calibrator = hasattr(analyzer, 'calibrator')
        has_truth_table_builder = hasattr(analyzer, 'truth_table_builder')
        has_minimizer = hasattr(analyzer, 'minimizer')
        
        print(f"  âœ“ å®šæ€§æ–¹æ³• - ç†è®ºåˆ†æ: {'âœ“' if has_theoretical_analysis else 'âœ—'}")
        print(f"  âœ“ å®šæ€§æ–¹æ³• - æ ¡å‡†æŒ‡å¯¼: {'âœ“' if has_calibration_guidance else 'âœ—'}")
        print(f"  âœ“ å®šæ€§æ–¹æ³• - ç»“æœè§£é‡Š: {'âœ“' if has_result_interpretation else 'âœ—'}")
        print(f"  âœ“ å®šé‡ç»„ä»¶ - æ ¡å‡†å™¨: {'âœ“' if has_calibrator else 'âœ—'}")
        print(f"  âœ“ å®šé‡ç»„ä»¶ - çœŸå€¼è¡¨æ„å»ºå™¨: {'âœ“' if has_truth_table_builder else 'âœ—'}")
        print(f"  âœ“ å®šé‡ç»„ä»¶ - æœ€å°åŒ–å™¨: {'âœ“' if has_minimizer else 'âœ—'}")
        
        results = {
            'valid': all([
                has_theoretical_analysis, 
                has_calibration_guidance, 
                has_result_interpretation,
                has_calibrator,
                has_truth_table_builder,
                has_minimizer
            ]),
            'qualitative_methods': {
                'theoretical_analysis': has_theoretical_analysis,
                'calibration_guidance': has_calibration_guidance,
                'result_interpretation': has_result_interpretation
            },
            'quantitative_components': {
                'calibrator': has_calibrator,
                'truth_table_builder': has_truth_table_builder,
                'minimizer': has_minimizer
            }
        }
        
    except Exception as e:
        print(f"  âœ— å®šæ€§å®šé‡ç»“åˆæœºåˆ¶: å¤±è´¥ - {str(e)}")
        results = {'valid': False, 'error': str(e)}
    
    print("  å®šæ€§å®šé‡ç»“åˆæœºåˆ¶æµ‹è¯•å®Œæˆ\n")
    return results


def generate_test_report(all_results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("ğŸ“„ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    
    report_path = Path(__file__).parent / 'comprehensive_test_report.json'
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ“ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # ç”Ÿæˆæ‘˜è¦
    total_tests = len(all_results)
    passed_tests = sum(1 for result in all_results.values() if result.get('valid', False))
    
    print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
    print(f"  æ€»æµ‹è¯•é¡¹: {total_tests}")
    print(f"  é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"  å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
    print(f"  æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
    
    return report_path


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ” å¼€å§‹fsQCAæŠ€èƒ½å…¨é¢ç³»ç»Ÿæµ‹è¯•\n")
    
    all_results = {}
    
    # 1. æµ‹è¯•æ ¡å‡†æ¨¡å—
    all_results['calibration'] = test_calibration_module()
    
    # 2. æµ‹è¯•çœŸå€¼è¡¨æ¨¡å—
    all_results['truth_table'] = test_truth_table_module()
    
    # 3. æµ‹è¯•æœ€å°åŒ–æ¨¡å—
    all_results['minimization'] = test_minimization_module()
    
    # 4. æµ‹è¯•é›†æˆåˆ†ææ¨¡å—
    all_results['integration'] = test_integration_module()
    
    # 5. æµ‹è¯•æ¸è¿›å¼æŠ«éœ²åŠŸèƒ½
    all_results['progressive_disclosure'] = test_progressive_disclosure()
    
    # 6. æµ‹è¯•å®šæ€§å®šé‡ç»“åˆæœºåˆ¶
    all_results['qualitative_quantitative'] = test_qualitative_quantitative_integration()
    
    # 7. æµ‹è¯•æ–‡æ¡£é“¾æ¥
    all_results['documentation'] = test_documentation_links()
    
    # 8. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    report_path = generate_test_report(all_results)
    
    print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹: {report_path}")
    
    return all_results


if __name__ == "__main__":
    main()