index,title,authors,summary,published,updated,arxiv_id,pdf_url,categories,doi,comment,journal_ref
1,A Tutorial about Random Neural Networks in Supervised Learning,Sebastián Basterrech; Gerardo Rubino,"Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can also be seen as a specific type of queuing network. They have been successfully used in several domains during the last 25 years, as queuing networks to analyze the performance of resource sharing in many engineering areas, as learning tools and in combinatorial optimization, where they are seen as neural systems, and also as models of neurological aspects of living beings. In this article we focus on their learning capabilities, and more specifically, we present a practical guide for using the RNN to solve supervised learning problems. We give a general description of these models using almost indistinctly the terminology of Queuing Theory and the neural one. We present the standard learning procedures used by RNNs, adapted from similar well-established improvements in the standard NN field. We describe in particular a set of learning algorithms covering techniques based on the use of first order and, then, of second order derivatives. We also discuss some issues related to these objects and present new perspectives about their use in supervised learning problems. The tutorial describes their most relevant applications, and also provides a large bibliography.",2016-09-15T20:21:30Z,2016-09-15T20:21:30Z,1609.04846v1,https://arxiv.org/pdf/1609.04846v1.pdf,cs.NE,10.14311/NNW.2015.25.024,This paper is a draft of an article to be published in Neural Network World,"Neural Network World, Volume 5, Number 15, pp.:457-499, 2015"
2,Predicting concentration levels of air pollutants by transfer learning and recurrent neural network,Iat Hang Fong; Tengyue Li; Simon Fong; Raymond K. Wong; Antonio J. Tallón-Ballesteros,"Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Additionally, meteorological data and data on the concentration of APS have been utilized. Moreover, in Macau, some air quality monitoring stations (AQMSs) have less observed data in quantity, and, at the same time, some AQMSs recorded less observed data of certain types of APS. Therefore, the transfer learning and pre-trained neural networks have been employed to assist AQMSs with less observed data to build a neural network with high prediction accuracy. The experimental sample covers a period longer than 12-year and includes daily measurements from several APS as well as other more classical meteorological values. Records from five stations, four out of them are AQMSs and the remaining one is an automatic weather station, have been prepared from the aforesaid period and eventually underwent to computational intelligence techniques to build and extract a prediction knowledge-based system. As shown by experimentation, LSTM RNNs initialized with transfer learning methods have higher prediction accuracy; it incurred shorter training time than randomly initialized recurrent neural networks.",2025-01-30T23:39:19Z,2025-01-30T23:39:19Z,2502.01654v1,https://arxiv.org/pdf/2502.01654v1.pdf,cs.LG; cs.NE; physics.ao-ph,,"Forecasting, environment monitoring, transfer learning, recurrent neural network, airborne particle matter","Fong, I. H., Li, T., Fong, S., Wong, R. K., & Tallon-Ballesteros, A. J. (2020). Predicting concentration levels of air pollutants by transfer learning and recurrent neural network. Knowledge-Based Systems, 192, 105622"
3,Masked Conditional Neural Networks for Audio Classification,Fady Medhat; David Chesmore; John Robinson,We present the ConditionaL Neural Network (CLNN) and the Masked ConditionaL Neural Network (MCLNN) designed for temporal signal recognition. The CLNN takes into consideration the temporal nature of the sound signal and the MCLNN extends upon the CLNN through a binary mask to preserve the spatial locality of the features and allows an automated exploration of the features combination analogous to hand-crafting the most relevant features for the recognition task. MCLNN has achieved competitive recognition accuracies on the GTZAN and the ISMIR2004 music datasets that surpass several state-of-the-art neural network based architectures and hand-crafted methods applied on both datasets.,2018-03-06T20:54:00Z,2019-03-23T10:56:33Z,1803.02421v2,https://arxiv.org/pdf/1803.02421v2.pdf,stat.ML; cs.LG; cs.SD; eess.AS,10.1007/978-3-319-68612-7_40,"Restricted BoltzmannMachine, RBM, Conditional Restricted Boltzmann Machine, CRBM, Music Information Retrieval, MIR, Conditional Neural Network, CLNN, Masked Conditional Neural Network, MCLNN, Deep Neural Network",International Conference on Artificial Neural Networks (ICANN) Year: 2017
4,The Deep Arbitrary Polynomial Chaos Neural Network or how Deep Artificial Neural Networks could benefit from Data-Driven Homogeneous Chaos Theory,Sergey Oladyshkin; Timothy Praditia; Ilja Kröker; Farid Mohammadi; Wolfgang Nowak; Sebastian Otte,"Artificial Intelligence and Machine learning have been widely used in various fields of mathematical computing, physical modeling, computational science, communication science, and stochastic analysis. Approaches based on Deep Artificial Neural Networks (DANN) are very popular in our days. Depending on the learning task, the exact form of DANNs is determined via their multi-layer architecture, activation functions and the so-called loss function. However, for a majority of deep learning approaches based on DANNs, the kernel structure of neural signal processing remains the same, where the node response is encoded as a linear superposition of neural activity, while the non-linearity is triggered by the activation functions. In the current paper, we suggest to analyze the neural signal processing in DANNs from the point of view of homogeneous chaos theory as known from polynomial chaos expansion (PCE). From the PCE perspective, the (linear) response on each node of a DANN could be seen as a $1^{st}$ degree multi-variate polynomial of single neurons from the previous layer, i.e. linear weighted sum of monomials. From this point of view, the conventional DANN structure relies implicitly (but erroneously) on a Gaussian distribution of neural signals. Additionally, this view revels that by design DANNs do not necessarily fulfill any orthogonality or orthonormality condition for a majority of data-driven applications. Therefore, the prevailing handling of neural signals in DANNs could lead to redundant representation as any neural signal could contain some partial information from other neural signals. To tackle that challenge, we suggest to employ the data-driven generalization of PCE theory known as arbitrary polynomial chaos (aPC) to construct a corresponding multi-variate orthonormal representations on each node of a DANN to obtain Deep arbitrary polynomial chaos neural networks.",2023-06-26T15:09:14Z,2023-06-26T15:09:14Z,2306.14753v1,https://arxiv.org/pdf/2306.14753v1.pdf,cs.NE; stat.ML,10.1016/j.neunet.2023.06.036,Submitted to Neural Networks,"Neural Networks Volume 166, September 2023, Pages 85-104"
5,Development of a sensory-neural network for medical diagnosing,Igor Grabec; Eva Švegl; Mihael Sok,Performance of a sensory-neural network developed for diagnosing of diseases is described. Information about patient's condition is provided by answers to the questionnaire. Questions correspond to sensors generating signals when patients acknowledge symptoms. These signals excite neurons in which characteristics of the diseases are represented by synaptic weights associated with indicators of symptoms. The disease corresponding to the most excited neuron is proposed as the result of diagnosing. Its reliability is estimated by the likelihood defined by the ratio of excitation of the most excited neuron and the complete neural network.,2018-07-06T16:30:53Z,2018-07-06T16:30:53Z,1807.02477v1,https://arxiv.org/pdf/1807.02477v1.pdf,cs.NE,,"International symposium on neural networks 2018, Minsk, Belarus, June 25-28, 2018","Advances in Neural Networks - ISNN 2018, pp. 91-98, Part of Springer ""Lecture Notes in Computer Science"" book series (LNCS, volume 10878)"
6,A Review on Neural Network Models of Schizophrenia and Autism Spectrum Disorder,Pablo Lanillos; Daniel Oliva; Anja Philippsen; Yuichi Yamashita; Yukie Nagai; Gordon Cheng,"This survey presents the most relevant neural network models of autism spectrum disorder and schizophrenia, from the first connectionist models to recent deep network architectures. We analyzed and compared the most representative symptoms with its neural model counterpart, detailing the alteration introduced in the network that generates each of the symptoms, and identifying their strengths and weaknesses. We additionally cross-compared Bayesian and free-energy approaches, as they are widely applied to modeling psychiatric disorders and share basic mechanisms with neural networks. Models of schizophrenia mainly focused on hallucinations and delusional thoughts using neural dysconnections or inhibitory imbalance as the predominating alteration. Models of autism rather focused on perceptual difficulties, mainly excessive attention to environment details, implemented as excessive inhibitory connections or increased sensory precision. We found an excessive tight view of the psychopathologies around one specific and simplified effect, usually constrained to the technical idiosyncrasy of the used network architecture. Recent theories and evidence on sensorimotor integration and body perception combined with modern neural network architectures could offer a broader and novel spectrum to approach these psychopathologies. This review emphasizes the power of artificial neural networks for modeling some symptoms of neurological disorders but also calls for further developing these techniques in the field of computational psychiatry.",2019-06-24T15:10:44Z,2019-10-23T09:25:59Z,1906.10015v2,https://arxiv.org/pdf/1906.10015v2.pdf,q-bio.NC; cs.AI; cs.NE,10.1016/j.neunet.2019.10.014,Preprint submitted to Neural Networks. Research not referenced in the manuscript within the field of NN models of SZ and ASD are encouraged to contact the corresponding authors,Neural Networks 122 (2020) 338-363
7,How transferable are features in deep neural networks?,Jason Yosinski; Jeff Clune; Yoshua Bengio; Hod Lipson,"Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",2014-11-06T23:09:37Z,2014-11-06T23:09:37Z,1411.1792v1,https://arxiv.org/pdf/1411.1792v1.pdf,cs.LG; cs.NE,,To appear in Advances in Neural Information Processing Systems 27 (NIPS 2014),"Advances in Neural Information Processing Systems 27, pages 3320-3328. Dec. 2014"
8,Parallel Neural Networks in Golang,Daniela Kalwarowskyj; Erich Schikuta,"This paper describes the design and implementation of parallel neural networks (PNNs) with the novel programming language Golang. We follow in our approach the classical Single-Program Multiple-Data (SPMD) model where a PNN is composed of several sequential neural networks, which are trained with a proportional share of the training dataset. We used for this purpose the MNIST dataset, which contains binary images of handwritten digits. Our analysis focusses on different activation functions and optimizations in the form of stochastic gradients and initialization of weights and biases. We conduct a thorough performance analysis, where network configurations and different performance factors are analyzed and interpreted. Golang and its inherent parallelization support proved very well for parallel neural network simulation by considerable decreased processing times compared to sequential variants.",2023-04-19T11:56:36Z,2023-04-19T11:56:36Z,2304.09590v1,https://arxiv.org/pdf/2304.09590v1.pdf,cs.NE; cs.DC,,"Extended version of paper Daniela Kalwarowskyj and Erich Schikuta, SPMD-based Neural Network Simulation with Golang, published at International Conference on Computational Science (ICCS) 2023",
9,Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation,Giorgio Morales; John W. Sheppard,"Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or ""high-quality"" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks automatically in addition to the conventional target predictions. In particular, we train two companion neural networks: one that uses one output, the target estimate, and another that uses two outputs, the upper and lower bounds of the corresponding PI. Our main contribution is the design of a novel loss function for the PI-generation network that takes into account the output of the target-estimation network and has two optimization objectives: minimizing the mean prediction interval width and ensuring the PI integrity using constraints that maximize the prediction interval probability coverage implicitly. Furthermore, we introduce a self-adaptive coefficient that balances both objectives within the loss function, which alleviates the task of fine-tuning. Experiments using a synthetic dataset, eight benchmark datasets, and a real-world crop yield prediction dataset showed that our method was able to maintain a nominal probability coverage and produce significantly narrower PIs without detriment to its target estimation accuracy when compared to those PIs generated by three state-of-the-art neural-network-based methods. In other words, our method was shown to produce higher-quality PIs.",2022-12-13T05:03:16Z,2024-03-22T02:30:02Z,2212.06370v4,https://arxiv.org/pdf/2212.06370v4.pdf,cs.LG; stat.ML,10.1109/TNNLS.2023.3339470,Accepted at the IEEE Transactions on Neural Networks and Learning Systems,"G. Morales and J. W. Sheppard, ""Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation,"" in IEEE Transactions on Neural Networks and Learning Systems, 2023"
10,Training a Hidden Markov Model with a Bayesian Spiking Neural Network,Amirhossein Tavanaei; Anthony S Maida,"It is of some interest to understand how statistically based mechanisms for signal processing might be integrated with biologically motivated mechanisms such as neural networks. This paper explores a novel hybrid approach for classifying segments of sequential data, such as individual spoken works. The approach combines a hidden Markov model (HMM) with a spiking neural network (SNN). The HMM, consisting of states and transitions, forms a fixed backbone with nonadaptive transition probabilities. The SNN, however, implements a biologically based Bayesian computation that derives from the spike timing-dependent plasticity (STDP) learning rule. The emission (observation) probabilities of the HMM are represented in the SNN and trained with the STDP rule. A separate SNN, each with the same architecture, is associated with each of the states of the HMM. Because of the STDP training, each SNN implements an expectation maximization algorithm to learn the emission probabilities for one HMM state. The model was studied on synthesized spike-train data and also on spoken word data. Preliminary results suggest its performance compares favorably with other biologically motivated approaches. Because of the model's uniqueness and initial promise, it warrants further study. It provides some new ideas on how the brain might implement the equivalent of an HMM in a neural circuit.",2016-06-02T19:48:22Z,2016-07-20T22:42:41Z,1606.00825v2,https://arxiv.org/pdf/1606.00825v2.pdf,cs.NE,,"Bayesian Spiking Neural Network, Revision submitted: April-27-2016","Journal of Signal Processing Systems, (2016), 1-10"
