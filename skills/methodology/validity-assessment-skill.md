---
name: assessing-research-validity
description: è¯„ä¼°ç ”ç©¶ä¿¡åº¦æ•ˆåº¦ï¼ŒåŒ…æ‹¬å†…åœ¨æ•ˆåº¦ã€å¤–åœ¨æ•ˆåº¦ã€æ„å¿µæ•ˆåº¦ã€ç»Ÿè®¡ç»“è®ºæ•ˆåº¦å’Œå†…å®¹æ•ˆåº¦åˆ†æã€‚å½“éœ€è¦è¯„ä¼°ç ”ç©¶è´¨é‡ã€æ£€éªŒç ”ç©¶è®¾è®¡ã€è§£å†³æ•ˆåº¦é—®é¢˜æˆ–åº”å¯¹å®¡ç¨¿äººè´¨ç–‘æ—¶ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚
---

# ç ”ç©¶ä¿¡åº¦æ•ˆåº¦è¯„ä¼°æŠ€èƒ½

## ğŸ¯ æ ¸å¿ƒç›®æ ‡ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
ä¸ºä¸­æ–‡ç¤¾ä¼šç§‘å­¦ç ”ç©¶æä¾›ç§‘å­¦ã€ç³»ç»Ÿçš„ä¿¡åº¦æ•ˆåº¦è¯„ä¼°æ”¯æŒï¼Œç¡®ä¿ç ”ç©¶ç»“æœçš„å¯é æ€§ã€å‡†ç¡®æ€§å’Œæ¨å¹¿æ€§ã€‚

## ğŸ“‹ å¿…é¡»é¦–å…ˆæŒæ¡çš„æ•ˆåº¦æ¦‚å¿µ

### 1. æ•ˆåº¦çš„å››å¤§ç±»å‹
**æœ€é‡è¦æ¦‚å¿µ**ï¼š
- **å†…åœ¨æ•ˆåº¦**ï¼šå› æœæ¨æ–­çš„å‡†ç¡®æ€§ï¼ˆå†…éƒ¨çœŸå®æ€§ï¼‰
- **å¤–åœ¨æ•ˆåº¦**ï¼šç»“æœæ¨å¹¿çš„é€‚ç”¨æ€§ï¼ˆå¤–éƒ¨çœŸå®æ€§ï¼‰
- **æ„å¿µæ•ˆåº¦**ï¼šæµ‹é‡çš„å‡†ç¡®æ€§ï¼ˆæ„å¿µçœŸå®æ€§ï¼‰
- **ç»Ÿè®¡ç»“è®ºæ•ˆåº¦**ï¼šç»Ÿè®¡æ¨æ–­çš„å¯é æ€§ï¼ˆç»Ÿè®¡çœŸå®æ€§ï¼‰

### 2. ä¸­æ–‡ç¤¾ç§‘ç ”ç©¶æ•ˆåº¦ç‰¹æ®Šæ€§
**å¿…é¡»è€ƒè™‘**ï¼š
- **æ–‡åŒ–æ•æ„Ÿæ€§**ï¼šä¸­å›½æ–‡åŒ–èƒŒæ™¯å¯¹æµ‹é‡çš„å½±å“
- **åˆ¶åº¦å› ç´ **ï¼šç¤¾ä¼šåˆ¶åº¦å¯¹ç ”ç©¶ç»“æœçš„åˆ¶çº¦
- **è¯­è¨€é€‚åˆ‡æ€§**ï¼šä¸­æ–‡è¡¨è¾¾å’Œç†è§£çš„å‡†ç¡®æ€§
- **å®è·µç›¸å…³æ€§**ï¼šç†è®ºä¸ä¸­å›½å®è·µçš„ç»“åˆåº¦

### 3. æ•ˆåº¦å¨èƒè¯†åˆ«åŸåˆ™
**å¿…é¡»è¯†åˆ«**ï¼š
- **é€‰æ‹©å¨èƒ**ï¼šæ ·æœ¬é€‰æ‹©åå·®
- **å†å²å¨èƒ**ï¼šæ—¶é—´åºåˆ—å˜åŒ–å½±å“
- **æˆç†Ÿå¨èƒ**ï¼šç ”ç©¶å¯¹è±¡è‡ªèº«å˜åŒ–
- **æµ‹è¯•å¨èƒ**ï¼šæµ‹é‡å·¥å…·çš„å½±å“
- **å·¥å…·å¨èƒ**ï¼šæµ‹é‡å·¥å…·çš„å±€é™æ€§

## ğŸ”„ åŠ¨æ€çŸ¥è¯†åº“åŠ è½½

### å¯åŠ¨æ—¶åŠ è½½
```
/knowledge-base/main-knowledge.md
/knowledge-base/core-concepts.md
/knowledge-base/validity-fundamentals.md
```

### æŒ‰éœ€åŠ è½½
```
ç”¨æˆ·éœ€è¦å†…åœ¨æ•ˆåº¦ â†’ /knowledge-base/internal-validity.md
ç”¨æˆ·éœ€è¦å¤–åœ¨æ•ˆåº¦ â†’ /knowledge-base/external-validity.md
ç”¨æˆ·éœ€è¦æ„å¿µæ•ˆåº¦ â†’ /knowledge-base/construct-validity.md
ç”¨æˆ·éœ€è¦ç»Ÿè®¡æ•ˆåº¦ â†’ /knowledge-base/statistical-validity.md
```

## ğŸš¨ ç´§æ€¥å¤„ç†åè®®

### çº¢è‰²è­¦æŠ¥ï¼ˆè®ºæ–‡å®¡ç¨¿ï¼‰
**å¿«é€Ÿè¯„ä¼°æ¨¡å¼**ï¼š
1. æä¾›æ•ˆåº¦æ£€æŸ¥æ¸…å•
2. å¿«é€Ÿè¯†åˆ«ä¸»è¦å¨èƒ
3. ç”Ÿæˆæ”¹è¿›å»ºè®®
4. æ‰¿è¯º48å°æ—¶å†…å®Œæˆè¯¦ç»†è¯„ä¼°

### é»„è‰²è­¦æŠ¥ï¼ˆå¯¼å¸ˆè¦æ±‚ï¼‰
**æ ‡å‡†è¯„ä¼°æ¨¡å¼**ï¼š
1. ç³»ç»Ÿæ€§å››ç»´åº¦æ•ˆåº¦è¯„ä¼°
2. è¯¦ç»†åˆ†æå¨èƒå› ç´ 
3. è®¾è®¡æ•ˆåº¦å¢å¼ºæ–¹æ¡ˆ
4. æä¾›å®æ–½æŒ‡å¯¼

## ğŸ› ï¸ æ ¸å¿ƒè¯„ä¼°æŠ€èƒ½

### 1. å†…åœ¨æ•ˆåº¦è¯„ä¼°
**æ ¸å¿ƒæŠ€èƒ½**ï¼š
```python
def assess_internal_validity(research_design):
    """è¯„ä¼°å†…åœ¨æ•ˆåº¦"""
    validity_threats = {
        'selection': {
            'threat_level': assess_selection_threat(research_design),
            'mitigation_strategies': [
                'éšæœºåˆ†é…',
                'åŒ¹é…è®¾è®¡',
                'å‰æµ‹-åæµ‹è®¾è®¡',
                'åå˜é‡æ§åˆ¶'
            ],
            'indicators': [
                'éšæœºåŒ–ç¨‹åº¦',
                'ç»„é—´ç­‰ä»·æ€§',
                'æ§åˆ¶å˜é‡æ•°é‡'
            ]
        },
        'history': {
            'threat_level': assess_history_threat(research_design),
            'mitigation_strategies': [
                'æ—¶é—´åºåˆ—è®¾è®¡',
                'é—´æ–­æ—¶é—´åºåˆ—è®¾è®¡',
                'å¤šé‡æ—¶é—´åºåˆ—è®¾è®¡',
                'æ§åˆ¶ç»„è®¾è®¡'
            ],
            'indicators': [
                'æ—¶é—´é—´éš”åˆç†æ€§',
                'å†å²æ•°æ®å®Œæ•´æ€§',
                'å¤–éƒ¨äº‹ä»¶è®°å½•'
            ]
        },
        'maturation': {
            'threat_level': assess_maturation_threat(research_design),
            'mitigation_strategies': [
                'ç¼©çŸ­ç ”ç©¶å‘¨æœŸ',
                'æ§åˆ¶æˆç†Ÿå½±å“',
                'æˆç†Ÿåº¦æµ‹é‡',
                'ç»Ÿè®¡æ§åˆ¶'
            ],
            'indicators': [
                'ç ”ç©¶å¯¹è±¡ç¨³å®šæ€§',
                'æˆç†Ÿåº¦å˜åŒ–è§„å¾‹',
                'æˆç†Ÿåº¦æµ‹é‡å·¥å…·'
            ]
        },
        'testing': {
            'threat_level': assess_testing_threat(research_design),
            'mitigation_strategies': [
                'æ ‡å‡†åŒ–å·¥å…·',
                'å·¥å…·ç­‰æ•ˆæ€§æ£€éªŒ',
                'åŒç›²è®¾è®¡',
                'å·¥å…·åŸ¹è®­'
            ],
            'indicators': [
                'å·¥å…·ä¿¡åº¦',
                'æµ‹è¯•ä¸€è‡´æ€§',
                'æµ‹è¯•è€…åŸ¹è®­æ•ˆæœ'
            ]
        },
        'instrumentation': {
            'threat_level': assess_instrumentation_threat(research_design),
            'mitigation_strategies': [
                'å·¥å…·æ ‡å‡†åŒ–',
                'é¢„æµ‹è¯•',
                'è·¨å·¥å…·æ¯”è¾ƒ',
                'å·¥å…·æ ¡å‡†'
            ],
            'indicators': [
                'å·¥å…·ä¿¡åº¦',
                'æµ‹é‡ç²¾åº¦',
                'å·¥å…·ç¨³å®šæ€§'
            ]
        }
    }
    
    return {
        'threats': validity_threats,
        'overall_assessment': calculate_overall_validity(validity_threats),
        'recommendations': generate_validity_recommendations(validity_threats)
    }

def assess_selection_threat(research_design):
    """è¯„ä¼°é€‰æ‹©å¨èƒ"""
    if research_design.get('sampling_method') == 'convenience':
        return 'high'
    elif research_design.get('sampling_method') == 'purposive':
        return 'medium'
    elif research_design.get('sampling_method') == 'random':
        return 'low'
    else:
        return 'unknown'

def calculate_overall_validity(threats):
    """è®¡ç®—æ•´ä½“æ•ˆåº¦"""
    threat_levels = [threat['threat_level'] for threat in threats.values()]
    
    # ç®€åŒ–è¯„ä¼°ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„ç®—æ³•ï¼‰
    if any(level == 'high' for level in threat_levels):
        return 'low'
    elif any(level == 'medium' for level in threat_levels):
        return 'medium'
    else:
        return 'high'
```

### 2. å¤–åœ¨æ•ˆåº¦è¯„ä¼°
**æ ¸å¿ƒæŠ€èƒ½**ï¼š
```python
def assess_external_validity(research_results):
    """è¯„ä¼°å¤–åœ¨æ•ˆåº¦"""
    validity_aspects = {
        'population_validity': {
            'target_population': research_results['target_population'],
            'sample_representativeness': assess_sample_representativeness(
                research_results['sample'], 
                research_results['target_population']
            ),
            'sampling_method': research_results['sampling_method']
        },
        'ecological_validity': {
            'real_world_setting': assess_setting_authenticity(research_results),
            'context_similarity': assess_context_similarity(research_results),
            'time_relevance': assess_time_relevance(research_results)
        },
        'temporal_validity': {
            'time_period_relevance': assess_time_relevance(research_results),
            'historical_context': assess_historical_context(research_results),
            'future_applicability': assess_future_applicability(research_results)
        },
        'cross_cultural_validity': {
            'cultural_applicability': assess_cultural_applicability(research_results),
            'language_applicability': assess_language_applicability(research_results),
            'contextual_relevance': assess_contextual_relevance(research_results)
        }
    }
    
    return {
        'aspects': validity_aspects,
        'overall_assessment': calculate_external_validity_score(validity_aspects),
        'generalizability': assess_generalizability(validity_aspects),
        'limitations': identify_external_limitations(validity_aspects)
    }

def assess_sample_representativeness(sample, population):
    """è¯„ä¼°æ ·æœ¬ä»£è¡¨æ€§"""
    # ç®€åŒ–è¯„ä¼°
    sample_size = len(sample)
    population_size = len(population)
    
    # æ£€æŸ¥å…³é”®ç‰¹å¾åˆ†å¸ƒ
    key_characteristics = ['age', 'gender', 'education', 'income', 'region']
    representativeness_scores = []
    
    for characteristic in key_characteristics:
        sample_dist = get_characteristic_distribution(sample, characteristic)
        population_dist = get_characteristic_distribution(population, characteristic)
        
        # è®¡ç®—åˆ†å¸ƒç›¸ä¼¼åº¦
        similarity = calculate_distribution_similarity(sample_dist, population_dist)
        representativeness_scores.append(similarity)
    
    return sum(representativeness_scores) / len(representativeness_scores)
```

### 3. æ„å¿µæ•ˆåº¦è¯„ä¼°
**æ ¸å¿ƒæŠ€èƒ½**ï¼š
```python
def assess_construct_validity(measurement_tools, research_concepts):
    """è¯„ä¼°æ„å¿µæ•ˆåº¦"""
    validity_assessment = {
        'content_validity': {
            'expert_validation': assess_expert_validation(measurement_tools, research_concepts),
            'face_validity': assess_face_validity(measurement_tools, research_concepts),
            'convergent_validity': assess_convergent_validity(measurement_tools, research_concepts),
            'discriminant_validity': assess_discriminant_validity(measurement_tools, research_concepts)
        },
        'criterion_validity': {
            'predictive_validity': assess_predictive_validity(measurement_tools, research_concepts),
            'concurrent_validity': assess_concurrent_validity(measurement_tools, research_concepts),
            'postdictive_validity': assess_postdictive_validity(measurement_tools, research_concepts)
        },
        'translation_validity': {
            'nomological_network': assess_nomological_network(measurement_tools),
            'convergent_discriminant': assess_convergent_discriminant(measurement_tools),
            'discriminant_convergent': assess_discriminant_convergent(measurement_tools)
        }
    }
    
    return {
        'assessment': validity_assessment,
        'overall_score': calculate_construct_validity_score(validity_assessment),
        'weaknesses': identify_construct_weaknesses(validity_assessment),
        'improvements': suggest_construct_improvements(validity_assessment)
    }

def assess_expert_validation(tools, concepts):
    """ä¸“å®¶æ•ˆåº¦è¯„ä¼°"""
    validation_results = {}
    
    for concept in concepts:
        # æ¨¡æ‹Ÿä¸“å®¶è¯„ä¼°ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦çœŸå®ä¸“å®¶å‚ä¸ï¼‰
        expert_rating = simulate_expert_rating(concept, tools)
        validation_results[concept] = expert_rating
    
    return validation_results

def simulate_expert_rating(concept, available_tools):
    """æ¨¡æ‹Ÿä¸“å®¶è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    # åŸºäºå·¥å…·è¦†ç›–åº¦å’Œæ¦‚å¿µå¤æ‚åº¦è¯„ä¼°
    tool_coverage = len([tool for tool in available_tools if concept in tool['measures']])
    concept_complexity = assess_concept_complexity(concept)
    
    # ç®€åŒ–è¯„åˆ†ç®—æ³•
    base_score = (tool_coverage / len(available_tools)) * 0.7
    complexity_adjustment = (concept_complexity / 10) * 0.3
    
    return min(base_score + complexity_adjustment, 1.0)
```

### 4. ç»Ÿè®¡ç»“è®ºæ•ˆåº¦è¯„ä¼°
**æ ¸å¿ƒæŠ€èƒ½**ï¼š
```python
def assess_statistical_conclusion(data_analysis):
    """è¯„ä¼°ç»Ÿè®¡ç»“è®ºæ•ˆåº¦"""
    validity_checks = {
        'assumption_testing': {
            'normality_assumption': assess_normality_assumption(data_analysis),
            'independence_assumption': assess_independence_assumption(data_analysis),
            'homogeneity_assumption': assess_homogeneity_assumption(data_analysis),
            'linearity_assumption': assess_linearity_assumption(data_analysis)
        },
        'statistical_power': {
            'power_analysis': calculate_statistical_power(data_analysis),
            'sample_size_adequacy': assess_sample_size_adequacy(data_analysis),
            'effect_size_meaning': assess_effect_size_meaning(data_analysis)
        },
        'multiple_comparisons': {
            'type_i_error_rate': calculate_type_i_error_rate(data_analysis),
            'familywise_error_rate': calculate_familywise_error_rate(data_analysis),
            'false_discovery_rate': calculate_false_discovery_rate(data_analysis)
        },
        'robustness_checks': {
            'sensitivity_analysis': perform_sensitivity_analysis(data_analysis),
            'outlier_influence': assess_outlier_influence(data_analysis),
            'model_specification': check_model_specification(data_analysis)
        }
    }
    
    return {
        'checks': validity_checks,
        'overall_assessment': calculate_statistical_validity(validity_checks),
        'recommendations': generate_statistical_recommendations(validity_checks),
        'report_templates': generate_statistical_report_templates(validity_checks)
    }

def assess_normality_assumption(data_analysis):
    """è¯„ä¼°æ­£æ€æ€§å‡è®¾"""
    test_stat, p_value = scipy.stats.shapiro(data_analysis['data'])
    
    if p_value > 0.05:
        return {'assumption_met': 'no', 'p_value': p_value, 'test_statistic': test_stat}
    else:
        return {'assumption_met': 'yes', 'p_value': p_value, 'test_statistic': test_stat}

def calculate_statistical_power(data_analysis):
    """è®¡ç®—ç»Ÿè®¡åŠŸæ•ˆ"""
    effect_size = data_analysis.get('effect_size')
    sample_size = data_analysis.get('sample_size')
    alpha = 0.05  # æ˜¾è‘—æ€§æ°´å¹³
    
    # ä½¿ç”¨Cohen's dçš„è¿‘ä¼¼åŠŸæ•ˆè®¡ç®—
    if effect_size is None or sample_size is None:
        return {'power': 'unknown', 'reason': 'Missing effect size or sample size'}
    
    # ç®€åŒ–çš„åŠŸæ•ˆè®¡ç®—
    z_alpha = 1.96  # åŒå°¾æ£€éªŒçš„ä¸´ç•Œå€¼
    n1 = sample_size / 2
    n2 = sample_size / 2
    
    power = 0.5 * (1 - norm.cdf(z_alpha - effect_size * math.sqrt(n1 * n2 / 2)))
    
    return {'power': power, 'effect_size': effect_size, 'sample_size': sample_size}
```

## ğŸ“Š è´¨é‡æ£€æŸ¥æ¸…å•

### æ•ˆåº¦è¯„ä¼°å®Œæ•´æ€§
- [ ] æ˜¯å¦è¿›è¡Œäº†å››ç»´åº¦æ•ˆåº¦è¯„ä¼°
- [ ] æ˜¯å¦è¯†åˆ«äº†ä¸»è¦å¨èƒå› ç´ 
- [ ] æ˜¯å¦æä¾›äº†æ”¹è¿›å»ºè®®
- [ ] æ˜¯å¦è€ƒè™‘äº†ä¸­æ–‡è¯­å¢ƒç‰¹æ®Šæ€§
- [ ] æ˜¯å¦æä¾›äº†å…·ä½“å®æ–½æ–¹æ¡ˆ

### ç»Ÿè®¡æ–¹æ³•ç§‘å­¦æ€§
- [ ] å‡è®¾æ£€éªŒæ˜¯å¦æ­£ç¡®æ‰§è¡Œ
- [ ] åŠŸæ•ˆåˆ†ææ˜¯å¦å……åˆ†
- [ ] å¤šé‡æ¯”è¾ƒæ˜¯å¦æ§åˆ¶
- [ ] é²¾æ£’æ€§æ£€éªŒæ˜¯å¦å®Œæˆ

### å®è·µå¯è¡Œæ€§
- [ ] æ”¹è¿›æ–¹æ¡ˆæ˜¯å¦å¯æ“ä½œ
- [ ] æˆæœ¬æ•ˆç›Šæ˜¯å¦åˆç†
- [ ] æ—¶é—´å®‰æ’æ˜¯å¦å¯è¡Œ
- [ ] æŠ€æœ¯è¦æ±‚æ˜¯å¦å¯è¾¾
- [ ] ä¼¦ç†è¦æ±‚æ˜¯å¦æ»¡è¶³

## ğŸ’¡ å¿«é€Ÿå“åº”æ¨¡æ¿

### ç´§æ€¥æ•ˆåº¦è¯„ä¼°æ¨¡æ¿
```
1. æä¾›æ•ˆåº¦æ£€æŸ¥æ¸…å•
2. å¿«é€Ÿè¯†åˆ«ä¸»è¦å¨èƒ
3. ç”Ÿæˆåˆæ­¥æ”¹è¿›å»ºè®®
4. æä¾›æ ‡å‡†åŒ–æŠ¥å‘Šæ¨¡æ¿
5. æ‰¿è¯º48å°æ—¶å†…å®Œæˆè¯¦ç»†è¯„ä¼°
```

### æ ‡å‡†æ•ˆåº¦è¯„ä¼°æ¨¡æ¿
```
1. ç³»ç»Ÿæ€§å››ç»´åº¦æ•ˆåº¦è¯„ä¼°
2. è¯¦ç»†åˆ†æå¨èƒå› ç´ 
3. è®¾è®¡æ•ˆåº¦å¢å¼ºæ–¹æ¡ˆ
4. ç”Ÿæˆå®Œæ•´è¯„ä¼°æŠ¥å‘Š
5. æä¾›å®æ–½æŒ‡å¯¼å’Œæ—¶é—´è¡¨
```

## ğŸ”§ æ•ˆåº¦å¢å¼ºç­–ç•¥

### å†…åœ¨æ•ˆåº¦å¢å¼º
```python
def enhance_internal_validity(research_design):
    """å¢å¼ºå†…åœ¨æ•ˆåº¦"""
    enhancements = []
    
    # éšæœºåŒ–å¢å¼º
    if research_design.get('randomization') != 'true':
        enhancements.append("å»ºè®®é‡‡ç”¨éšæœºåˆ†é…æ–¹æ³•")
    
    # åŒ¹é…è®¾è®¡å¢å¼º
    if research_design.get('matching') != 'true':
        enhancements.append("å»ºè®®é‡‡ç”¨åŒ¹é…è®¾è®¡")
    
    # å‰åæµ‹è®¾è®¡å¢å¼º
    if not research_design.get('pretest'):
        enhancements.append("å»ºè®®å¢åŠ å‰æµ‹")
    
    return enhancements
```

### å¤–åœ¨æ•ˆåº¦å¢å¼º
```python
def enhance_external_validity(research_design):
    """å¢å¼ºå¤–åœ¨æ•ˆåº¦"""
    enhancements = []
    
    # æ ·æœ¬å¤šæ ·æ€§å¢å¼º
    if research_design.get('sample_diversity') == 'low':
        enhancements.append("å»ºè®®å¢åŠ æ ·æœ¬å¤šæ ·æ€§")
    
    # å¤šåœ°ç‚¹ç ”ç©¶
    if not research_design.get('multi_site'):
        enhancements.append("å»ºè®®è€ƒè™‘å¤šåœ°ç‚¹ç ”ç©¶")
    
    # å†å²æ¯”è¾ƒç ”ç©¶
    if not research_design.get('historical_comparison'):
        enhancements.append("å»ºè®®æ·»åŠ å†å²æ¯”è¾ƒ")
    
    return enhancements
```

---

**ä½¿ç”¨è¯´æ˜**ï¼šæ­¤æŠ€èƒ½ä¸¥æ ¼éµå¾ªç¤¾ä¼šç§‘å­¦ç ”ç©¶æ•ˆåº¦è¯„ä¼°è§„èŒƒï¼Œæä¾›ç§‘å­¦ã€ç³»ç»Ÿçš„æ•ˆåº¦åˆ†ææ”¯æŒã€‚