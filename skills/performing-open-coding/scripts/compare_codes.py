#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒç»­æ¯”è¾ƒè„šæœ¬ - å¼€æ”¾ç¼–ç 

åŠŸèƒ½ï¼š
- è®¡ç®—ç¼–ç é—´ç›¸ä¼¼åº¦
- è¯†åˆ«é‡å¤ç¼–ç 
- å»ºè®®åˆå¹¶ç¼–ç 

ä½¿ç”¨æ–¹å¼ï¼š
  python compare_codes.py --input codes.json --output comparison.json
"""

import argparse
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

def calculate_similarity(code1: str, code2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªç¼–ç çš„è¯­ä¹‰ç›¸ä¼¼åº¦
    
    Args:
        code1: ç¼–ç 1
        code2: ç¼–ç 2
    
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
    """
    # ä½¿ç”¨TF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦
    vectorizer = TfidfVectorizer(tokenizer=lambda x: jieba.lcut(x))
    try:
        tfidf_matrix = vectorizer.fit_transform([code1, code2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity)
    except:
        return 0.0

def identify_duplicates(codes: List[Dict], threshold: float = 0.8) -> List[Tuple]:
    """
    è¯†åˆ«é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ç¼–ç 
    
    Args:
        codes: ç¼–ç åˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.8ï¼‰
    
    Returns:
        é‡å¤ç¼–ç å¯¹åˆ—è¡¨ [(code1, code2, similarity), ...]
    """
    duplicates = []
    n = len(codes)
    
    for i in range(n):
        for j in range(i + 1, n):
            # å…¼å®¹'code'å’Œ'concept'å­—æ®µ
            code1 = codes[i].get('code') or codes[i].get('concept', '')
            code2 = codes[j].get('code') or codes[j].get('concept', '')
            
            if not code1 or not code2:
                continue
            
            similarity = calculate_similarity(code1, code2)
            
            if similarity >= threshold:
                duplicates.append({
                    'code1': code1,
                    'code2': code2,
                    'similarity': round(similarity, 3),
                    'index1': i,
                    'index2': j
                })
    
    return duplicates

def suggest_merges(codes: List[Dict], duplicates: List[Dict]) -> List[Dict]:
    """
    å»ºè®®åˆå¹¶ç¼–ç 
    
    Args:
        codes: ç¼–ç åˆ—è¡¨
        duplicates: é‡å¤ç¼–ç åˆ—è¡¨
    
    Returns:
        åˆå¹¶å»ºè®®åˆ—è¡¨
    """
    suggestions = []
    
    for dup in duplicates:
        code1_data = codes[dup['index1']]
        code2_data = codes[dup['index2']]
        
        # é€‰æ‹©é¢‘ç‡æ›´é«˜çš„ä½œä¸ºä¸»ç¼–ç 
        if code1_data.get('frequency', 0) >= code2_data.get('frequency', 0):
            primary = code1_data
            secondary = code2_data
        else:
            primary = code2_data
            secondary = code1_data
        
        # å…¼å®¹å­—æ®µå
        primary_code = primary.get('code') or primary.get('concept', '')
        secondary_code = secondary.get('code') or secondary.get('concept', '')
        
        suggestions.append({
            'action': 'merge',
            'primary_code': primary_code,
            'secondary_code': secondary_code,
            'similarity': dup['similarity'],
            'reason': f"ç›¸ä¼¼åº¦ {dup['similarity']:.1%}ï¼Œå»ºè®®åˆå¹¶",
            'combined_frequency': primary.get('frequency', 0) + secondary.get('frequency', 0)
        })
    
    return suggestions

def analyze_code_relationships(codes: List[Dict]) -> Dict:
    """
    åˆ†æç¼–ç é—´çš„å…³ç³»
    
    Returns:
        å…³ç³»åˆ†æç»“æœ
    """
    n = len(codes)
    similarity_matrix = np.zeros((n, n))
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    for i in range(n):
        for j in range(i + 1, n):
            code_i = codes[i].get('code') or codes[i].get('concept', '')
            code_j = codes[j].get('code') or codes[j].get('concept', '')
            if not code_i or not code_j:
                continue
            sim = calculate_similarity(code_i, code_j)
            similarity_matrix[i][j] = sim
            similarity_matrix[j][i] = sim
    
    # æ‰¾å‡ºæ¯ä¸ªç¼–ç æœ€ç›¸å…³çš„ç¼–ç 
    relationships = []
    for i in range(n):
        related_indices = np.argsort(similarity_matrix[i])[::-1][1:4]  # å‰3ä¸ªç›¸å…³ç¼–ç 
        related_codes = [
            {
                'code': codes[j].get('code') or codes[j].get('concept', ''),
                'similarity': round(float(similarity_matrix[i][j]), 3)
            }
            for j in related_indices if similarity_matrix[i][j] > 0.3
        ]
        
        if related_codes:
            relationships.append({
                'code': codes[i].get('code') or codes[i].get('concept', ''),
                'related_codes': related_codes
            })
    
    return {
        'similarity_matrix': similarity_matrix.tolist(),
        'relationships': relationships
    }

def main():
    parser = argparse.ArgumentParser(
        description='ç¼–ç æŒç»­æ¯”è¾ƒå·¥å…· - å¼€æ”¾ç¼–ç ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python compare_codes.py --input codes.json --output comparison.json
  python compare_codes.py -i codes.json -o result.json --threshold 0.75
        """
    )
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥çš„ç¼–ç JSONæ–‡ä»¶')
    parser.add_argument('--output', '-o', default='comparison.json', help='è¾“å‡ºJSONæ–‡ä»¶')
    parser.add_argument('--threshold', '-t', type=float, default=0.8, help='ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ï¼š0.8ï¼‰')
    args = parser.parse_args()
    
    start_time = time.time()
    
    try:
        # è¯»å–ç¼–ç æ•°æ®
        input_path = Path(args.input)
        if not input_path.exists():
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            sys.exit(1)
        
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–ç¼–ç åˆ—è¡¨
        if 'details' in data and 'concepts' in data['details']:
            codes = data['details']['concepts']
        elif isinstance(data, list):
            codes = data
        else:
            logging.error("æ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼")
            sys.exit(2)
        
        if len(codes) < 2:
            logging.error("ç¼–ç æ•°é‡ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦2ä¸ªï¼‰")
            sys.exit(3)
        
        logging.info(f"âœ“ è¯»å–ç¼–ç : {len(codes)} ä¸ª")
        
        # æ‰§è¡Œæ¯”è¾ƒåˆ†æ
        duplicates = identify_duplicates(codes, args.threshold)
        suggestions = suggest_merges(codes, duplicates)
        relationships = analyze_code_relationships(codes)
        
        processing_time = time.time() - start_time
        
        # æ„å»ºè¾“å‡º
        output = {
            'summary': {
                'total_codes': len(codes),
                'duplicate_pairs': len(duplicates),
                'merge_suggestions': len(suggestions),
                'similarity_threshold': args.threshold,
                'processing_time': round(processing_time, 2)
            },
            'details': {
                'duplicates': duplicates,
                'merge_suggestions': suggestions,
                'relationships': relationships['relationships'][:20]  # å‰20ä¸ªå…³ç³»
            },
            'metadata': {
                'input_file': str(input_path.absolute()),
                'timestamp': datetime.now().isoformat(),
                'version': '1.0.0'
            }
        }
        
        # ä¿å­˜
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        logging.info(f"âœ… æ¯”è¾ƒåˆ†æå®Œæˆ")
        logging.info(f"   ç¼–ç æ€»æ•°: {len(codes)}")
        logging.info(f"   é‡å¤å¯¹æ•°: {len(duplicates)}")
        logging.info(f"   åˆå¹¶å»ºè®®: {len(suggestions)}")
        logging.info(f"ğŸ“„ è¯¦ç»†ç»“æœ: {args.output}")
        
    except Exception as e:
        logging.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(99)

if __name__ == "__main__":
    main()
