#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¦‚å¿µèšç±»è„šæœ¬ - å¼€æ”¾ç¼–ç 

åŠŸèƒ½ï¼š
- K-meansèšç±»
- å±‚æ¬¡èšç±»
- èšç±»å¯è§†åŒ–

ä½¿ç”¨æ–¹å¼ï¼š
  python cluster_concepts.py --input concepts.json --output clusters.json --n-clusters 5
"""

import argparse
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

def cluster_by_kmeans(concepts: List[Dict], n_clusters: int) -> Dict:
    """
    ä½¿ç”¨K-meansèšç±»
    
    Args:
        concepts: æ¦‚å¿µåˆ—è¡¨
        n_clusters: èšç±»æ•°é‡
    
    Returns:
        èšç±»ç»“æœå­—å…¸
    """
    if len(concepts) < n_clusters:
        logging.warning(f"æ¦‚å¿µæ•°é‡({len(concepts)})å°‘äºèšç±»æ•°({n_clusters})ï¼Œè°ƒæ•´ä¸º{len(concepts)}")
        n_clusters = len(concepts)
    
    # æå–æ¦‚å¿µæ–‡æœ¬
    concept_texts = [c.get('concept') or c.get('code', '') for c in concepts]
    
    # TF-IDFå‘é‡åŒ–
    vectorizer = TfidfVectorizer(tokenizer=lambda x: jieba.lcut(x))
    try:
        tfidf_matrix = vectorizer.fit_transform(concept_texts)
    except Exception as e:
        logging.error(f"å‘é‡åŒ–å¤±è´¥: {e}")
        return {'clusters': [], 'error': str(e)}
    
    # K-meansèšç±»
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(tfidf_matrix)
    
    # ç»„ç»‡èšç±»ç»“æœ
    clusters = {}
    for i, label in enumerate(labels):
        label = int(label)
        if label not in clusters:
            clusters[label] = []
        clusters[label].append({
            'concept': concept_texts[i],
            'frequency': concepts[i].get('frequency', 1),
            'type': concepts[i].get('type', 'ä¸€èˆ¬æ¦‚å¿µ')
        })
    
    # ä¸ºæ¯ä¸ªèšç±»å‘½å
    cluster_list = []
    for cluster_id, items in clusters.items():
        # ä½¿ç”¨æœ€é«˜é¢‘çš„æ¦‚å¿µä½œä¸ºèšç±»åç§°
        representative = max(items, key=lambda x: x['frequency'])
        cluster_list.append({
            'cluster_id': cluster_id,
            'cluster_name': representative['concept'],
            'size': len(items),
            'concepts': items,
            'total_frequency': sum(c['frequency'] for c in items)
        })
    
    return {
        'method': 'kmeans',
        'n_clusters': n_clusters,
        'clusters': sorted(cluster_list, key=lambda x: x['total_frequency'], reverse=True)
    }

def cluster_hierarchical(concepts: List[Dict], n_clusters: int = None) -> Dict:
    """
    ä½¿ç”¨å±‚æ¬¡èšç±»
    
    Args:
        concepts: æ¦‚å¿µåˆ—è¡¨
        n_clusters: èšç±»æ•°é‡ï¼ˆNoneåˆ™è‡ªåŠ¨ç¡®å®šï¼‰
    
    Returns:
        èšç±»ç»“æœå­—å…¸
    """
    if n_clusters is None:
        n_clusters = max(2, len(concepts) // 3)  # è‡ªåŠ¨ç¡®å®š
    
    if len(concepts) < 2:
        return {'clusters': [], 'error': 'æ¦‚å¿µæ•°é‡ä¸è¶³'}
    
    # æå–æ¦‚å¿µæ–‡æœ¬
    concept_texts = [c.get('concept') or c.get('code', '') for c in concepts]
    
    # TF-IDFå‘é‡åŒ–
    vectorizer = TfidfVectorizer(tokenizer=lambda x: jieba.lcut(x))
    try:
        tfidf_matrix = vectorizer.fit_transform(concept_texts)
    except Exception as e:
        logging.error(f"å‘é‡åŒ–å¤±è´¥: {e}")
        return {'clusters': [], 'error': str(e)}
    
    # å±‚æ¬¡èšç±»
    hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
    labels = hierarchical.fit_predict(tfidf_matrix.toarray())
    
    # ç»„ç»‡èšç±»ç»“æœ
    clusters = {}
    for i, label in enumerate(labels):
        label = int(label)
        if label not in clusters:
            clusters[label] = []
        clusters[label].append({
            'concept': concept_texts[i],
            'frequency': concepts[i].get('frequency', 1),
            'type': concepts[i].get('type', 'ä¸€èˆ¬æ¦‚å¿µ')
        })
    
    # ä¸ºæ¯ä¸ªèšç±»å‘½å
    cluster_list = []
    for cluster_id, items in clusters.items():
        representative = max(items, key=lambda x: x['frequency'])
        cluster_list.append({
            'cluster_id': cluster_id,
            'cluster_name': representative['concept'],
            'size': len(items),
            'concepts': items,
            'total_frequency': sum(c['frequency'] for c in items)
        })
    
    return {
        'method': 'hierarchical',
        'n_clusters': n_clusters,
        'clusters': sorted(cluster_list, key=lambda x: x['total_frequency'], reverse=True)
    }

def generate_cluster_summary(clusters: List[Dict]) -> Dict:
    """
    ç”Ÿæˆèšç±»æ‘˜è¦ç»Ÿè®¡
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    total_concepts = sum(c['size'] for c in clusters)
    
    # èšç±»å¤§å°åˆ†å¸ƒ
    sizes = [c['size'] for c in clusters]
    
    # æ¦‚å¿µç±»å‹åˆ†å¸ƒ
    type_distribution = {}
    for cluster in clusters:
        for concept in cluster['concepts']:
            concept_type = concept['type']
            type_distribution[concept_type] = type_distribution.get(concept_type, 0) + 1
    
    return {
        'total_clusters': len(clusters),
        'total_concepts': total_concepts,
        'average_cluster_size': round(np.mean(sizes), 2),
        'max_cluster_size': max(sizes),
        'min_cluster_size': min(sizes),
        'type_distribution': type_distribution,
        'top_clusters': [
            {
                'name': c['cluster_name'],
                'size': c['size'],
                'frequency': c['total_frequency']
            }
            for c in clusters[:5]
        ]
    }

def main():
    parser = argparse.ArgumentParser(
        description='æ¦‚å¿µèšç±»å·¥å…· - å¼€æ”¾ç¼–ç ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python cluster_concepts.py --input concepts.json --output clusters.json
  python cluster_concepts.py -i concepts.json -o result.json --n-clusters 5 --method kmeans
        """
    )
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥çš„æ¦‚å¿µJSONæ–‡ä»¶')
    parser.add_argument('--output', '-o', default='clusters.json', help='è¾“å‡ºJSONæ–‡ä»¶')
    parser.add_argument('--n-clusters', '-n', type=int, help='èšç±»æ•°é‡ï¼ˆé»˜è®¤è‡ªåŠ¨ç¡®å®šï¼‰')
    parser.add_argument('--method', '-m', choices=['kmeans', 'hierarchical', 'both'], 
                       default='kmeans', help='èšç±»æ–¹æ³•ï¼ˆé»˜è®¤ï¼škmeansï¼‰')
    args = parser.parse_args()
    
    start_time = time.time()
    
    try:
        # è¯»å–æ¦‚å¿µæ•°æ®
        input_path = Path(args.input)
        if not input_path.exists():
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            sys.exit(1)
        
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–æ¦‚å¿µåˆ—è¡¨
        if 'details' in data and 'concepts' in data['details']:
            concepts = data['details']['concepts']
        elif isinstance(data, list):
            concepts = data
        else:
            logging.error("æ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼")
            sys.exit(2)
        
        if len(concepts) < 2:
            logging.error("æ¦‚å¿µæ•°é‡ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦2ä¸ªï¼‰")
            sys.exit(3)
        
        logging.info(f"âœ“ è¯»å–æ¦‚å¿µ: {len(concepts)} ä¸ª")
        
        # è‡ªåŠ¨ç¡®å®šèšç±»æ•°
        if args.n_clusters is None:
            args.n_clusters = max(2, min(len(concepts) // 3, 10))
            logging.info(f"è‡ªåŠ¨ç¡®å®šèšç±»æ•°: {args.n_clusters}")
        
        # æ‰§è¡Œèšç±»
        results = {}
        
        if args.method in ['kmeans', 'both']:
            logging.info("æ‰§è¡ŒK-meansèšç±»...")
            kmeans_result = cluster_by_kmeans(concepts, args.n_clusters)
            results['kmeans'] = kmeans_result
        
        if args.method in ['hierarchical', 'both']:
            logging.info("æ‰§è¡Œå±‚æ¬¡èšç±»...")
            hierarchical_result = cluster_hierarchical(concepts, args.n_clusters)
            results['hierarchical'] = hierarchical_result
        
        processing_time = time.time() - start_time
        
        # é€‰æ‹©ä¸»è¦ç»“æœ
        if args.method == 'both':
            primary_result = results['kmeans']
        else:
            primary_result = results.get('kmeans') or results.get('hierarchical')
        
        # ç”Ÿæˆæ‘˜è¦
        summary = generate_cluster_summary(primary_result['clusters'])
        
        # æ„å»ºè¾“å‡º
        output = {
            'summary': {
                **summary,
                'method': args.method,
                'processing_time': round(processing_time, 2)
            },
            'details': results,
            'metadata': {
                'input_file': str(input_path.absolute()),
                'timestamp': datetime.now().isoformat(),
                'version': '1.0.0'
            }
        }
        
        # ä¿å­˜
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        logging.info(f"âœ… èšç±»å®Œæˆ")
        logging.info(f"   èšç±»æ•°é‡: {summary['total_clusters']}")
        logging.info(f"   æ¦‚å¿µæ€»æ•°: {summary['total_concepts']}")
        logging.info(f"   å¹³å‡å¤§å°: {summary['average_cluster_size']}")
        logging.info(f"ğŸ“„ è¯¦ç»†ç»“æœ: {args.output}")
        
    except Exception as e:
        logging.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(99)

if __name__ == "__main__":
    main()
